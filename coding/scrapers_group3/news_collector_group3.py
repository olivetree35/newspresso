#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Group 3: ë™ì  ì›¹ ìˆ˜ì§‘ê¸° with ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ìº¡ì²˜ (ê°œì„ ë¨)
- ì‹¤ì œ PDF ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ (ë„¤íŠ¸ì›Œí¬ ì‘ë‹µ ëª¨ë‹ˆí„°ë§)
- ìƒˆ íƒ­ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
- ì‚¬ì´íŠ¸ëª…, ì œëª©, ë‚ ì§œ, PDF ë‹¤ìš´ë¡œë“œ URL ìˆ˜ì§‘
"""

import asyncio
import importlib
import logging
from datetime import datetime
import re
from urllib.parse import urljoin

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

RESEARCH_SITES = [
    {
        'site_name': 'LHí† ì§€ì£¼íƒì—°êµ¬ì›',
        'url': 'https://lhri.lh.or.kr/web/pblictn/PblictnList.do?menuIdx=516&pblictnCode=LHRI_FOCUS',
        'title_selector': 'td a',
        'date_selector': '.date',
        'pdf_link_selector': 'a[href*="atchFile"], a[href*=".pdf"]',
    },
    {
        'site_name': 'í•œêµ­ê¸ˆìœµì—°êµ¬ì›',
        'url': 'https://www.kif.re.kr/kif4/publication/pub_list?mid=20',
        'title_selector': 'h3',
        'date_selector': 'span.date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'NICEì‹ ìš©í‰ê°€',
        'url': 'https://www.nicerating.com/research/researchAll.do',
        'title_selector': 'h3, h4, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="download"], a[href*="pdf"]',
    },
    {
        'site_name': 'KDI',
        'url': 'https://eiec.kdi.re.kr/policy/materialList.do?depth1=A0000&depth2=A0600',
        'title_selector': 'h3',
        'date_selector': 'li span',
        'pdf_link_selector': 'a[href*="file"]',
    },
    {
        'site_name': 'ëŒ€í•œê±´ì„¤ì •ì±…ì—°êµ¬ì›',
        'url': 'https://www.ricon.re.kr/board/list.php?group=issue&page=economic_index&cate=9',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="file"], a[href*="pdf"]',
    },
    {
        'site_name': 'LH (ì¸ì‚¬ì´íŠ¸)',
        'url': 'https://lhri.lh.or.kr/web/pblictn/PblictnList.do?menuIdx=346&pblictnCode=LH_INSITE',
        'title_selector': 'td a',
        'date_selector': '.date',
        'pdf_link_selector': 'a[href*="atchFile"], a[href*=".pdf"]',
    },
    {
        'site_name': 'í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ',
        'url': 'https://www.hanaif.re.kr/totalSearch.do?srchNm=KYWD&srchKey=%EB%B6%80%EB%8F%99%EC%82%B0',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'í¬ìŠ¤ì½”ê²½ì˜ì—°êµ¬ì›',
        'url': 'https://www.posri.re.kr/kor/bbs/report_list.do?mmcd=2402221432440016120&cate=2403071010350015910',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a, button',
    },
    {
        'site_name': 'ì£¼íƒê¸ˆìœµì—°êµ¬ì›',
        'url': 'https://researcher.hf.go.kr/researcher/sub02/sub02_05.do',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="file"], a[href*="pdf"]',
    },
    {
        'site_name': 'ì„œìš¸ì—°êµ¬ì›',
        'url': 'https://www.si.re.kr/bbs/list.do?key=2024100039',
        'title_selector': 'h3',
        'date_selector': '.date',
        'pdf_link_selector': 'a[href*="file"]',
    },
    {
        'site_name': 'êµ­í† ì—°êµ¬ì› (ë¼ì´ë¸ŒëŸ¬ë¦¬)',
        'url': 'https://www.krihs.re.kr/krihsLibraryArticle/articleList.es?mid=a10103010000&pub_kind=1',
        'title_selector': 'td a',
        'date_selector': 'td:nth-child(3)',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'êµ­í† ì—°êµ¬ì› (ë³´ë“œ)',
        'url': 'https://www.krihs.re.kr/board.es?mid=a10607000000&bid=0008',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'LGê²½ì˜ì—°êµ¬ì›',
        'url': 'https://www.lgbr.co.kr/economy/list.do',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'í•œêµ­ê±´ì„¤ì‚°ì—…ì—°êµ¬ì› (ì‹œì¥ì „ë§)',
        'url': 'https://www.cerik.re.kr/material/prospect',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'í•œêµ­ê±´ì„¤ì‚°ì—…ì—°êµ¬ì› (ë™í–¥ë¸Œë¦¬í•‘)',
        'url': 'https://www.cerik.re.kr/report/briefing#/',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'í˜„ëŒ€ê²½ì œì—°êµ¬ì›',
        'url': 'https://www.hri.co.kr/kor/report/report.html?mode=1',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'KDI (í† í”½)',
        'url': 'https://www.kdi.re.kr/research/topicList?cd=A',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ (ë³´ë“œ)',
        'url': 'https://www.hanaif.re.kr/boardList.do?menuId=MN2000&tabMenuId=MN2100',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'ìš°ë¦¬ê¸ˆìœµì—°êµ¬ì†Œ',
        'url': 'https://www.wfri.re.kr/ko/web/research_report/research_report.php?search_type=list',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'ëŒ€ì‹ ì¦ê¶Œ',
        'url': 'https://money2.daishin.com/E5/ResearchCenter/Work/DW_ResearchReits.aspx?m=10904&p=11112&v=11661',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'KBê¸ˆìœµì§€ì£¼',
        'url': 'https://www.kbfg.com/kbresearch/report/reportList.do',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'BKL',
        'url': 'https://www.bkl.co.kr/law/insight/legalDataList?pageIndex=1&whichOne=NEWSLETTER',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ (ë³´ê³ ì„œ)',
        'url': 'https://www.hanaif.re.kr/boardList.do?menuId=MN1000&tabMenuId=MN1109',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'IBKê²½ì œì—°êµ¬ì†Œ',
        'url': 'http://research.ibk.co.kr/research/board/economy-news/list',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'ìº ì½”',
        'url': 'https://www.kamco.or.kr/portal/bbs/list.do?ptIdx=282&mId=0701030000',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'CUSHMAN & WAKEFIELD',
        'url': 'https://www.cushmanwakefield.com/ko-kr/south-korea/insights/insight-search',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    {
        'site_name': 'êµë³´ë¦¬ì–¼ì½”',
        'url': 'https://www.kyoborealco.co.kr/insight/marketreport',
        'title_selector': 'h4, h5, .title',
        'date_selector': 'span.date, .date',
        'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    },
    # {
    #     'site_name': 'CBRE',
    #     'url': 'https://www.cbrekorea.com/insights#%EC%9D%B8%EC%82%AC%EC%9D%B4%ED%8A%B8',
    #     'title_selector': 'h4, h5, .title',
    #     'date_selector': 'span.date, .date',
    #     'pdf_link_selector': 'a[href*="pdf"], a[href*="download"]',
    # }
]

class DynamicResearchCollector:
    """ë™ì  ì›¹ ìˆ˜ì§‘ê¸° - ë„¤íŠ¸ì›Œí¬ ì‘ë‹µ ìº¡ì²˜ (ê°œì„ ë¨)"""
    
    def __init__(self, start_date, end_date):
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d").date()
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d").date()
        self.results = []
        self.recent_responses = []
    
    async def collect_from_site(self, page, site_config, browser):
        """ë‹¨ì¼ ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ (ê°œì„ ë¨)"""
        collected = 0
        
        try:
            def _on_response(response):
                if response.status == 200:
                    url = response.url.lower()
                    if any(keyword in url for keyword in ['.pdf', 'download', 'atchfile', 'filedown', 'file']):
                        self.recent_responses.append(url)
                if len(self.recent_responses) > 200:
                    del self.recent_responses[:50]
            
            page.on("response", _on_response)

            await page.goto(site_config['url'], wait_until='domcontentloaded', timeout=15000)
            await page.wait_for_timeout(2000)
            
            title_elements = await page.query_selector_all(site_config['title_selector'])
            
            if not title_elements:
                logger.warning(f"[{site_config['site_name']}] í•­ëª© ì—†ìŒ")
                return collected
            
            for idx, title_elem in enumerate(title_elements[:20]):
                try:
                    title_text = await title_elem.text_content()
                    if not title_text or len(title_text.strip()) < 3:
                        continue
                    
                    date_text = "N/A"
                    try:
                        date_elem = await title_elem.evaluate_handle(f"""
                            el => el.closest('tr, li, article, div[class*="item"]')?.querySelector('{site_config["date_selector"]}')
                        """)
                        if date_elem:
                            date_text = await date_elem.text_content()
                    except Exception as e:
                        logger.debug(f"[{site_config['site_name']}] ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}")
                    
                    pdf_url = "N/A"
                    try:
                        pdf_link = await title_elem.evaluate_handle(f"""
                            el => el.closest('tr, li, article, div[class*="item"]')?.querySelector('{site_config["pdf_link_selector"]}')
                        """)
                        
                        if pdf_link:
                            pdf_href = await pdf_link.get_attribute('href')
                            
                            if pdf_href and ('pdf' in pdf_href.lower() or 'download' in pdf_href.lower()):
                                pdf_url = urljoin(site_config['url'], pdf_href)
                            else:
                                try:
                                    try:
                                        async with browser.context.expect_page(timeout=3000) as event:
                                            await pdf_link.click()
                                            new_tab = await event.value
                                            await new_tab.wait_for_load_state('networkidle', timeout=10000)
                                            pdf_url = new_tab.url
                                            await new_tab.close()
                                    except:
                                        await pdf_link.click()
                                        await page.wait_for_timeout(5000)
                                        
                                        for res_url in reversed(self.recent_responses[-100:]):
                                            if any(keyword in res_url.lower() for keyword in ['.pdf', 'download', 'atchfile', 'filedown', 'file']):
                                                pdf_url = res_url
                                                break
                                except Exception as e:
                                    logger.debug(f"[{site_config['site_name']}] PDF ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}")
                    except Exception as e:
                        logger.debug(f"[{site_config['site_name']}] PDF ì¶”ì¶œ ì˜¤ë¥˜: {str(e)[:50]}")
                    
                    article = {
                        'source': site_config['site_name'],
                        'title': title_text.strip()[:100],
                        'date': date_text.strip() if date_text != "N/A" else "N/A",
                        'page_url': site_config['url'],
                        'pdf_url': pdf_url,
                        'collected_at': datetime.now().isoformat()
                    }
                    self.results.append(article)
                    collected += 1
                
                except Exception as e:
                    logger.debug(f"[{site_config['site_name']}] í•­ëª© {idx} ì˜¤ë¥˜: {str(e)[:50]}")
                    continue
        
        except Exception as e:
            logger.error(f"[{site_config['site_name']}] ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)[:100]}")
        
        return collected

    async def _collect_savills(self, context):
        """Savills ì „ìš© ìˆ˜ì§‘ (ì»¤ìŠ¤í…€ ìŠ¤í¬ë˜í¼ ì‚¬ìš©)"""
        try:
            SavillsScraper = importlib.import_module("savills_scraper").SavillsScraper
        except Exception:
            SavillsScraper = importlib.import_module(".savills_scraper", package=__package__).SavillsScraper

        start_date = self.start_date.strftime("%Y-%m-%d")
        end_date = self.end_date.strftime("%Y-%m-%d")
        scraper = SavillsScraper(start_date, end_date)
        page = await scraper._setup_page(context)

        try:
            collected = await scraper.scrape(page)
        finally:
            await page.close()

        if scraper.results:
            self.results.extend(scraper.results)

        return collected
    
    async def _collect_cbre(self, context):
        """CBRE ì „ìš© ìˆ˜ì§‘ (ëª©ë¡ -> ìƒì„¸ í˜ì´ì§€ ì´ë™ ë°©ì‹)"""
        collected = 0
        logger.info("ğŸ“„ CBRE ìˆ˜ì§‘ ì‹œì‘...")
        
        try:
            page = await context.new_page()
            # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ì†
            url = "https://www.cbrekorea.com/insights"
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(3000)
            
            # 2. ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘
            # /insights/reports/ íŒ¨í„´ì„ ê°€ì§„ ë§í¬ ì°¾ê¸°
            anchors = await page.query_selector_all("a[href*='/insights/reports/']")
            urls = []
            for a in anchors:
                href = await a.get_attribute("href")
                if href and href not in urls:
                    urls.append(urljoin(url, href))
            
            logger.info(f"   ğŸ” CBRE ë¦¬í¬íŠ¸ ìƒì„¸ ë§í¬ {len(urls)}ê°œ ë°œê²¬")
            
            # 3. ê° ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ìˆ˜ì§‘
            for detail_url in urls[:15]: # ìƒìœ„ 15ê°œë§Œ ìš°ì„  ì‹œë„
                try:
                    logger.info(f"      ğŸ“– ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {detail_url}")
                    await page.goto(detail_url, wait_until='networkidle', timeout=20000)
                    await page.wait_for_timeout(2000)
                    
                    # ì œëª© ì¶”ì¶œ: h1 ë˜ëŠ” .cbre-c-article-header__title
                    title = await page.inner_text("h1")
                    title = title.strip() if title else "No Title"
                    
                    # ë‚ ì§œ ì¶”ì¶œ: .cbre-c-article-header__date ë˜ëŠ” 20XX íŒ¨í„´
                    date_text = "0000-00-00"
                    body_text = await page.inner_text("body")
                    date_match = re.search(r'20\d{2}[.-]\d{1,2}[.-]\d{1,2}', body_text)
                    if date_match:
                        date_text = date_match.group(0).replace('.', '-')
                    
                    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸° (ì‚¬ìš©ì íŒíŠ¸)
                    dl_button = await page.query_selector("a.cbre-c-download")
                    if dl_button:
                        pdf_url = await dl_button.get_attribute("href")
                        if pdf_url:
                            pdf_url = urljoin(detail_url, pdf_url.strip())
                            
                            self.results.append({
                                'source': 'CBRE',
                                'title': title,
                                'date': date_text,
                                'page_url': detail_url,
                                'pdf_url': pdf_url,
                                'collected_at': datetime.now().isoformat()
                            })
                            collected += 1
                            logger.info(f"         âœ… ìˆ˜ì§‘ ì„±ê³µ: {title[:20]}... ({date_text})")
                        else:
                            logger.warning(f"         âš ï¸ ë‹¤ìš´ë¡œë“œ ë§í¬ href ì—†ìŒ")
                    else:
                        logger.warning(f"         âš ï¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼(a.cbre-c-download)ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                except Exception as e:
                    logger.error(f"         [CBRE] ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
             logger.error(f"   âŒ CBRE ì „ì²´ ì—ëŸ¬: {e}")
        finally:
             await page.close()
             
        return collected
    
    async def collect_all(self):
        async_playwright = importlib.import_module("playwright.async_api").async_playwright

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36",
                viewport={'width': 1920, 'height': 1080},
                ignore_https_errors=True
            )
            
            print("\nğŸ“š ë™ì  ì›¹ ìˆ˜ì§‘ ì‹œì‘ (ë„¤íŠ¸ì›Œí¬ ì‘ë‹µ ìº¡ì²˜ - ê°œì„ ë¨)")
            print(f"ğŸ“… ê¸°ê°„: {self.start_date} ~ {self.end_date}")
            print("=" * 70)
            
            # 1. ì¼ë°˜ ì‚¬ì´íŠ¸ ìˆ˜ì§‘ (CBRE ì œì™¸ë¨)
            for site_config in RESEARCH_SITES:
                print(f"ğŸ“„ {site_config['site_name']} ìˆ˜ì§‘ ì¤‘...", end=" ", flush=True)
                page = await context.new_page()
                
                collected = await self.collect_from_site(page, site_config, browser)
                print(f"âœ… {collected}ê±´")
                
                await page.close()

            # 2. Savills ìˆ˜ì§‘
            print(f"ğŸ“„ Savills ìˆ˜ì§‘ ì¤‘...", end=" ", flush=True)
            savills_collected = await self._collect_savills(context)
            print(f"âœ… {savills_collected}ê±´")
            
            # 3. CBRE ìˆ˜ì§‘ (New)
            print(f"ğŸ“„ CBRE ìˆ˜ì§‘ ì¤‘...", end=" ", flush=True)
            cbre_collected = await self._collect_cbre(context)
            print(f"âœ… {cbre_collected}ê±´")
            
            await context.close()
            await browser.close()
        
        print("=" * 70)
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(self.results)}ê±´\n")
        return self.results

async def main():
    try:
        while True:
            try:
                start_date = input("\nìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD): ").strip()
                datetime.strptime(start_date, "%Y-%m-%d")
                break
            except ValueError:
                print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        while True:
            try:
                end_date = input("ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD): ").strip()
                end_dt = datetime.strptime(end_date, "%Y-%m-%d")
                start_dt = datetime.strptime(start_date, "%Y-%m-%d")
                if end_dt >= start_dt:
                    break
                else:
                    print("âŒ ì¢…ë£Œì¼ì´ ì‹œì‘ì¼ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.")
            except ValueError:
                print("âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        collector = DynamicResearchCollector(start_date, end_date)
        results = await collector.collect_all()
        
        if results:
            print("\n" + "=" * 130)
            print(f"{'ìˆ˜ì§‘ ê²°ê³¼ (ìƒìœ„ 20ê±´)':<130}")
            print("=" * 130)
            
            for i, article in enumerate(results[:20], 1):
                title_with_url = f"{article['title'][:60]}({article.get('page_url', article['source'])})"
                date_str = article['date'] if article['date'] != "N/A" else "ë¯¸ìƒ"
                site_name = article['source']
                pdf_url = article.get('pdf_url', 'N/A')
                
                if pdf_url == "N/A":
                    pdf_display = "(ë¯¸ì¶”ì¶œ)"
                else:
                    pdf_display = pdf_url[:60]
                
                print(f"\n#{i}")
                print(f"   ì œëª©: {title_with_url}")
                print(f"   ë‚ ì§œ: {date_str}")
                print(f"   ì‚¬ì´íŠ¸: {site_name}")
                print(f"   PDF URL: {pdf_display}")
            
            print("\n" + "=" * 130)
            print(f"ì´ {len(results)}ê±´ ìˆ˜ì§‘ë¨ | ë™ì  ë Œë”ë§ + ë„¤íŠ¸ì›Œí¬ ì‘ë‹µ ìº¡ì²˜")
            print("=" * 130)
        else:
            print("\nâš ï¸  í•´ë‹¹ ê¸°ê°„ì— ìˆ˜ì§‘ëœ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìê°€ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())

