#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„œìš¸ì—°êµ¬ì› (SI) ìŠ¤í¬ë˜í¼
- URL: https://www.si.re.kr/bbs/list.do?key=2024100039
- ë°©ì‹: Playwrightë¥¼ ì´ìš©í•œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (Referer/Cookie ë¬¸ì œ í•´ê²°)
"""

import sys
import os
import asyncio
import logging
import csv
import json
import re
from datetime import datetime
from urllib.parse import urljoin
import requests
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("SI")

class SIScraper:
    def __init__(self, start_date=None, end_date=None):
        self.base_url = "https://www.si.re.kr"
        # ë„ì‹œê³„íš/ì£¼íƒ í•„í„°(subject=003) ì ìš©
        self.target_url = "https://www.si.re.kr/bbs/list.do?key=2024100039&subject=003"
        
        self.start_date = self._parse_date(start_date)
        self.end_date = self._parse_date(end_date)
        
        self.results = []
        self.output_dir = os.path.join(os.path.dirname(__file__), "output")
        # ìƒìœ„ ë””ë ‰í† ë¦¬(output) êµ¬ì¡° ë§ì¶”ê¸°
        if not os.path.exists(self.output_dir):
             self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "output"))
             if not os.path.exists(self.output_dir):
                 self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "output"))
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œ í´ë”
        self.download_dir = os.path.join(self.output_dir, "downloads_si")
        os.makedirs(self.download_dir, exist_ok=True)

    def _parse_date(self, date_str):
        if not date_str: return None
        try: return datetime.strptime(date_str, "%Y-%m-%d")
        except: return None

    def _is_in_period(self, date_str):
        if not date_str: return False
        try:
            date_str = date_str.strip().replace('.', '-').replace('/', '-')
            match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
            if match:
                current_date = datetime.strptime(match.group(0), "%Y-%m-%d")
            else:
                return False
            if self.start_date and current_date < self.start_date: return False
            if self.end_date and current_date > self.end_date: return False
            return True
        except: return False

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            context = await browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                accept_downloads=True
            )
            page = await context.new_page()

            logger.info(f"ğŸŒ ì ‘ì† ì¤‘: {self.target_url}")
            await page.goto(self.target_url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(2000)

            max_pages = 20
            current_page = 1
            
            while current_page <= max_pages:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page} - ëª©ë¡ ë¶„ì„ ì¤‘...")
                # í•„í„° ì ìš© ì‹œ .result-list í´ë˜ìŠ¤ê°€ ì—†ì„ ìˆ˜ ìˆìŒ. ë²”ìš©ì ì¸ li:has(.txt-wrap) ì‚¬ìš©
                items = await page.query_selector_all('li:has(.txt-wrap)')
                if not items:
                    # í´ë°±: ì¼ë°˜ì ì¸ ë³´ë“œ ë¦¬ìŠ¤íŠ¸
                    items = await page.query_selector_all('.board_list li')
                
                if not items:
                    logger.warning("âŒ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    await page.screenshot(path="debug_si_final.png")
                    with open("debug_si_final.html", "w", encoding="utf-8") as f:
                        f.write(await page.content())
                    break
                
                logger.info(f"   ğŸ” ê²Œì‹œë¬¼ {len(items)}ê°œ ë°œê²¬")
                page_collected_count = 0
                
                for item in items:
                    # ì œëª© (strong.tit ì¶”ê°€)
                    title_elem = await item.query_selector('strong.tit, h3, .subject, .title, a.sbj')
                    if not title_elem: continue
                    title_text = (await title_elem.text_content()).strip()

                    # ë‚ ì§œ (i.date + span ìš°ì„ )
                    date_text = "0000-00-00"
                    
                    # 1ìˆœìœ„: i.date + span (2025-05-23)
                    date_elem = await item.query_selector('i.date + span')
                    if date_elem:
                        date_text = (await date_elem.text_content()).strip()
                    else:
                        # 2ìˆœìœ„: .date, .reg_date (ì´ ê²½ìš° 'ë“±ë¡ì¼'ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜)
                        date_elem = await item.query_selector('.date, .reg_date')
                        if date_elem:
                             txt = (await date_elem.text_content()).strip()
                             # 'ë“±ë¡ì¼' í…ìŠ¤íŠ¸ë©´ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì •ê·œì‹ìœ¼ë¡œ
                             if "ë“±ë¡" not in txt:
                                 date_text = txt

                    if date_text == "0000-00-00":
                        txt = await item.text_content()
                        match = re.search(r'\d{4}[\.-]\d{2}[\.-]\d{2}', txt)
                        if match:
                            date_text = match.group(0)

                    # ë‚ ì§œ í¬ë§·íŒ…
                    date_text = date_text.replace('.', '-')
                    match_date = re.search(r'\d{4}-\d{2}-\d{2}', date_text)
                    if match_date:
                        date_text = match_date.group(0)
                        
                    logger.info(f"      [DEBUG] ì œëª©: {title_text[:20]}... | ë‚ ì§œ: {date_text}")

                    if not self._is_in_period(date_text): 
                        logger.info(f"      [SKIP] ê¸°ê°„ ë¯¸í•´ë‹¹: {date_text}")
                        continue
                    

                    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸°
                    download_link = await item.query_selector('a[href*="fileDown.do"]')
                    
                    pdf_result = "N/A"
                    abs_dl_url = ""
                    
                    if download_link:
                        try:
                            # URL ë° í—¤ë” ì •ë³´ ì¤€ë¹„
                            dl_href = await download_link.get_attribute('href')
                            abs_dl_url = urljoin(page.url, dl_href)
                            
                            # íŒŒì¼ëª… ìƒì„±
                            safe_title = re.sub(r'[\\/*?:"<>|]', "", title_text)
                            safe_title = safe_title[:50] 
                            filename = f"[SI]_{date_text}_{safe_title}.pdf"
                            save_path = os.path.join(self.download_dir, filename)
                            
                            # ì¿ í‚¤ ê°€ì ¸ì˜¤ê¸° context.cookies()ëŠ” í˜„ì¬ url ê¸°ì¤€
                            cookies = await context.cookies(self.target_url)
                            cookie_dict = {c['name']: c['value'] for c in cookies}
                            
                            # í—¤ë” ì„¤ì • (Referer í•„ìˆ˜)
                            headers = {
                                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                                "Referer": page.url
                            }

                            if os.path.exists(save_path) and os.path.getsize(save_path) > 1024:
                                logger.info(f"      â­ï¸ ì´ë¯¸ ì¡´ì¬í•¨ (Skipping): {filename}")
                                pdf_result = save_path
                            else:
                                logger.info(f"      ğŸ“¥ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œë„: {filename}")
                                response = requests.get(abs_dl_url, headers=headers, cookies=cookie_dict, verify=False, stream=True, timeout=60)
                                if response.status_code == 200:
                                    # ë‚´ìš©ì´ ì—ëŸ¬í˜ì´ì§€ì¸ì§€ í™•ì¸ (HTMLì´ë©´ ì‹¤íŒ¨)
                                    ct = response.headers.get('Content-Type', '').lower()
                                    if 'html' in ct:
                                        logger.warning("      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (HTML ì‘ë‹µ - ì°¨ë‹¨ë¨)")
                                        pdf_result = "DOWNLOAD_BLOCKED_HTML"
                                    else:
                                        with open(save_path, 'wb') as f:
                                            for chunk in response.iter_content(chunk_size=8192):
                                                f.write(chunk)
                                        logger.info("      âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                                        pdf_result = save_path
                                else:
                                    logger.warning(f"      âŒ HTTP ì—ëŸ¬: {response.status_code}")
                                    pdf_result = f"HTTP_{response.status_code}"

                        except Exception as e:
                            logger.warning(f"      âŒ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬: {e}")
                            pdf_result = "DOWNLOAD_FAILED"
                    else:
                        logger.info("      Link ì—†ìŒ")

                    if pdf_result and "FAILED" not in pdf_result and "BLOCKED" not in pdf_result:
                        self.results.append({
                            'source': 'SI',
                            'title': title_text,
                            'date': date_text,
                            'local_path': pdf_result,
                            'download_url': abs_dl_url,
                            'referer': page.url,
                            'collected_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        })
                        page_collected_count += 1
                
                # í˜ì´ì§€ë„¤ì´ì…˜
                try:
                    next_page = current_page + 1
                    # onclick="fn_egov_link_page(2)" í˜•ì‹ì´ ë§ìŒ
                    next_btn = await page.query_selector(f'a[href*="pageIndex={next_page}"], a[onclick*="{next_page}"]')
                    
                    if not next_btn:
                         next_btn = await page.query_selector('a.next, a.btn_next')

                    if next_btn:
                        await next_btn.click()
                        await page.wait_for_timeout(2000)
                        current_page += 1
                    else:
                        logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                        break
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ ì´ë™ ì¤‘ ì—ëŸ¬: {e}")
                    break

            await browser.close()
            self.save_files()

    def save_files(self):
        if not self.results:
            logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = os.path.join(self.output_dir, f"si_results_{timestamp}.csv")
        json_path = os.path.join(self.output_dir, f"si_results_{timestamp}.json")
        
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=self.results[0].keys())
            writer.writeheader()
            writer.writerows(self.results)
            
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)
            
        logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(self.results)}ê±´")
        logger.info(f"   - CSV: {csv_path}")
        logger.info(f"   - íŒŒì¼ìœ„ì¹˜: {self.download_dir}")

def get_user_date_range():
    print("\n[ì„œìš¸ì—°êµ¬ì› ìŠ¤í¬ë˜í¼ ì„¤ì •]")
    today = datetime.now().strftime("%Y-%m-%d")
    s = input(f"ì‹œì‘ ë‚ ì§œ (ì—”í„°: 2024-01-01): ").strip()
    if not s: s = "2024-01-01"
    e = input(f"ì¢…ë£Œ ë‚ ì§œ (ì—”í„°: {today}): ").strip()
    if not e: e = today
    return s, e

if __name__ == "__main__":
    if len(sys.argv) == 3:
        s, e = sys.argv[1], sys.argv[2]
    else:
        s, e = get_user_date_range()
    
    scraper = SIScraper(s, e)
    asyncio.run(scraper.scrape())
