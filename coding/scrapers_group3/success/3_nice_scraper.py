import asyncio
import os
import csv
import json
import logging
import re
import sys
import argparse
from datetime import datetime
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class NICEScraper:
    def __init__(self, start_date: str, end_date: str):
        self.base_url = "https://www.nicerating.com"
        # 'í˜„í–‰ í‰ê°€ë°©ë²•ë¡ ' í•„í„°ë¥¼ URL íŒŒë¼ë¯¸í„°ë¡œ ì§ì ‘ ì ìš©
        self.target_url = "https://www.nicerating.com/research/researchAll.do?fileTypM=230-1"
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d")
        self.results = []
        
    def _is_in_period(self, date_str: str) -> bool:
        """ë‚ ì§œê°€ ìˆ˜ì§‘ ê¸°ê°„ ë‚´ì¸ì§€ í™•ì¸"""
        try:
            target_date = datetime.strptime(date_str, "%Y-%m-%d")
            return self.start_date <= target_date <= self.end_date
        except:
            return False

    async def scrape(self):
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            logger.info(f"ì ‘ì† ì¤‘: {self.target_url}")
            await page.goto(self.target_url, wait_until="networkidle", timeout=60000)
            await page.wait_for_timeout(3000)

            total_collected = 0
            max_pages = 10 # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€
            
            for current_page in range(1, max_pages + 1):
                logger.info(f"í˜ì´ì§€ {current_page} ë¶„ì„ ì¤‘...")
                
                # ê²Œì‹œë¬¼ í…Œì´ë¸” ë¡œë”© ëŒ€ê¸°
                try:
                    await page.wait_for_selector('table.sortTable tbody tr', timeout=5000)
                except:
                    logger.info("ê²Œì‹œë¬¼ì´ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤ (Timeout).")
                    break
                
                rows = await page.query_selector_all('table.sortTable tbody tr')
                if not rows:
                    logger.info("ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤ (Empty rows).")
                    break

                page_count = 0
                for row in rows:
                    try:
                        # ì œëª© ì¶”ì¶œ
                        title_elem = await row.query_selector("td.cell_type01 a")
                        if not title_elem:
                            continue
                        title = (await title_elem.text_content()).strip()

                        # ë‚ ì§œ ì¶”ì¶œ
                        date_str = "N/A"
                        tds = await row.query_selector_all('td')
                        for td in tds:
                            txt = (await td.text_content()).strip()
                            if re.match(r"\d{4}\.\d{2}\.\d{2}", txt): # 2026.01.28
                                date_str = txt.replace('.', '-')
                                break
                        
                        # ê¸°ê°„ í•„í„°
                        if self.end_date < datetime.strptime(date_str, "%Y-%m-%d"):
                            # ì•„ì§ ê¸°ê°„ ì „ì„ (ê³„ì†)
                            continue
                        if self.start_date > datetime.strptime(date_str, "%Y-%m-%d"):
                            # ê¸°ê°„ ì§€ë‚¨ (ê·¸ë§Œí•´ë„ ë˜ì§€ë§Œ ìˆœì„œ ë³´ì¥ ì•ˆë˜ë©´ ê³„ì†)
                            logger.info(f"  [Skip] ë‚ ì§œ ë²”ìœ„ ë²—ì–´ë‚¨: {date_str}")
                            # ë‚ ì§œìˆœ ì •ë ¬ì´ë¼ë©´ ì—¬ê¸°ì„œ break ê°€ëŠ¥í•˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ continue for now
                            continue

                        # PDF URL ì¶”ì¶œ
                        row_html = await row.inner_html()
                        pdf_url = "N/A"
                        match = re.search(r"fncFileDown\(['\"]([^'\"]+)['\"]\)", row_html)
                        
                        if match:
                            doc_id = match.group(1)
                            pdf_url = f"https://www.nicerating.com/common/fileDown.do?docId={doc_id}"

                        # ê²°ê³¼ ì €ì¥
                        self.results.append({
                            "title": title,
                            "date": date_str,
                            "link": pdf_url, 
                            "source": "NICEì‹ ìš©í‰ê°€"
                        })
                        page_count += 1
                        total_collected += 1
                        logger.info(f"  [ìˆ˜ì§‘] {date_str} | {title[:30]}... | PDF: {bool(pdf_url!='N/A')}")

                    except Exception as e:
                        logger.error(f"  í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

                if page_count == 0 and current_page > 1:
                    # ì´ë²ˆ í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘í•œê²Œ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ (ë‚ ì§œ í•„í„° ë“±ìœ¼ë¡œ)
                    # ë§Œì•½ ë‚ ì§œ ì •ë ¬ì´ ë˜ì–´ìˆë‹¤ë©´ ì¢…ë£Œí•´ë„ ë¨.
                    # ì¼ë‹¨ ê³„ì† ì§„í–‰
                    pass

                # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
                if current_page < max_pages:
                    try:
                        next_page = current_page + 1
                        
                        # goPage í•¨ìˆ˜ ì‹¤í–‰
                        logger.info(f"í˜ì´ì§€ {next_page}ë¡œ ì´ë™ ì‹œë„...")
                        await page.evaluate(f"if (typeof goPage === 'function') {{ goPage({next_page}); }}")
                        
                        # ë¡œë”© ëŒ€ê¸°
                        await page.wait_for_timeout(3000) 
                        
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                        break
            
            await browser.close()
            logger.info(f"ì´ {total_collected}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            self.save_files()

    def save_files(self):
        if not self.results:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "scrapers_group3/output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # CSV ì €ì¥
        csv_filename = f"{output_dir}/nice_{timestamp}.csv"
        try:
            with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=["source", "title", "date", "link"])
                writer.writeheader()
                writer.writerows(self.results)
            logger.info(f"CSV ì €ì¥ ì™„ë£Œ: {csv_filename}")
        except Exception as e:
            logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            
        # JSON ì €ì¥
        json_filename = f"{output_dir}/nice_{timestamp}.json"
        try:
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            logger.info(f"JSON ì €ì¥ ì™„ë£Œ: {json_filename}")
        except Exception as e:
            logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    import sys
    
    start_date = None
    end_date = None
    
    # 1. ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        # 2. í„°ë¯¸ë„ ì…ë ¥ ì§€ì›
        print("\n" + "="*50)
        print("NICE ì‹ ìš©í‰ê°€ ìŠ¤í¬ë˜í¼ (Playwright)")
        print("="*50)
        
        try:
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            
            end_in = input("ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            sys.exit(0)
        
    try:
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        datetime.strptime(start_date, "%Y-%m-%d")
        datetime.strptime(end_date, "%Y-%m-%d")
    except ValueError:
        print(f"âŒ ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {start_date}, {end_date}")
        return

    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    print("ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    scraper = NICEScraper(start_date, end_date)
    asyncio.run(scraper.scrape())

if __name__ == "__main__":
    main()
