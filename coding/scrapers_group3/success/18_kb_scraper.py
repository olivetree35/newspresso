#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KBê¸ˆìœµì§€ì£¼ (KB Research) ìŠ¤í¬ë˜í¼
- URL: https://www.kbfg.com/kbresearch/report/reportList.do
- êµ¬ì¡°: ëª©ë¡ -> ìƒì„¸(ì„ íƒ) -> PDF ë‹¤ìš´ë¡œë“œ (JS fn_downFile í˜¸ì¶œ)
- íŠ¹ì§•: fn_downFile('FILE_ID', 'FILE_SN') -> /cmm/fms/FileDown.do?atchFileId=...&fileSn=...
"""

import sys
import os
import asyncio
import logging
import re
from datetime import datetime
from urllib.parse import urljoin
from playwright.async_api import async_playwright

# ìƒìœ„ í´ë” ê²½ë¡œ ì„¤ì • (base.py í˜¸ì¶œìš©)
cur_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(cur_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

try:
    from success.base import AsyncBaseScraper
except ImportError:
    sys.path.append(cur_dir)
    try:
        from base import AsyncBaseScraper
    except ImportError:
        print("âŒ base.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

logger = logging.getLogger("KBScraper")

class KBScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        super().__init__(start_date, end_date, site_name="KBê¸ˆìœµì§€ì£¼")
        self.base_url = "https://www.kbfg.com/kbresearch/"
        # ëª¨ë“  ë¦¬í¬íŠ¸ ëª©ë¡ (íŒŒë¼ë¯¸í„° ì—†ì´ ì ‘ê·¼ ì‹œ ì „ì²´ ëª©ë¡ ì˜ˆìƒ)
        self.target_url = "https://www.kbfg.com/kbresearch/report/reportList.do"

    async def _scrape_board(self, page, url):
        await page.goto(url, wait_until='networkidle', timeout=30000)
        
        collected_count = 0
        current_page = 1
        max_pages = 20 # ì•ˆì „ì¥ì¹˜
        
        while current_page <= max_pages:
            logger.info(f"   ğŸ“„ í˜ì´ì§€ {current_page} ì½ëŠ” ì¤‘...")
            await page.wait_for_timeout(1000)
            
            # ëª©ë¡ ì•„ì´í…œ ì¶”ì¶œ
            # ë³´í†µ div.boardList > ul > li ë˜ëŠ” table tr êµ¬ì¡°
            # êµ¬ì²´ì ì¸ selectorëŠ” í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥´ë‚˜, title ë§í¬(reportView.do)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì°¾ìŒ
            items = await page.query_selector_all('li:has(a[href*="reportView.do"]), tr:has(a[href*="reportView.do"])')
            
            # ë§Œì•½ ëª©ë¡ì—ì„œ ê°ì§€ê°€ ì•ˆë˜ë©´ ìƒì„¸í•˜ê²Œ ì°¾ê¸°
            if not items:
                # KB ResearchëŠ” ë³´í†µ ì¸ë„¤ì¼í˜•(ul.reportList > li) ë˜ëŠ” ë¦¬ìŠ¤íŠ¸í˜•ì„.
                items = await page.query_selector_all('.reportList > li, .boardList tr')
            
            if not items:
                logger.warning("   âš ï¸ ëª©ë¡ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
                
            logger.info(f"   â†’ ì•„ì´í…œ {len(items)}ê°œ ë°œê²¬")
            
            count_in_page = 0
            
            # ë°ì´í„° ì„ í–‰ ì¶”ì¶œ (DOM ë§Œë£Œ ë°©ì§€)
            extracted_items = []
            for item in items:
                try:
                    # ì œëª©
                    title_elem = await item.query_selector('a[href*="reportView.do"]')
                    if not title_elem: continue
                    title = (await title_elem.text_content()).strip()
                    view_href = await title_elem.get_attribute('href')
                    
                    # ë‚ ì§œ
                    date_elem = await item.query_selector('.date, .regDate, td:nth-child(4), dl dd, dd')
                    date_text = "0000-00-00"
                    if date_elem:
                        txt = (await date_elem.text_content()).strip()
                        m_date = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', txt)
                        if m_date:
                            date_text = f"{m_date.group(1)}-{m_date.group(2)}-{m_date.group(3)}"
                    else:
                        # ë‚ ì§œê°€ í…ìŠ¤íŠ¸ì— ì„ì—¬ ìˆì„ ìˆ˜ë„ ìˆìŒ
                        raw_txt = (await item.text_content()).strip()
                        m_date = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', raw_txt)
                        if m_date:
                            date_text = f"{m_date.group(1)}-{m_date.group(2)}-{m_date.group(3)}"
                            
                    # ë‹¤ìš´ë¡œë“œ ë§í¬(JS)
                    # href ë˜ëŠ” onclick í™•ì¸
                    down_btn = await item.query_selector('a[href*="fn_downFile"], a[onclick*="fn_downFile"]')
                    down_js = None
                    if down_btn:
                        href = await down_btn.get_attribute('href')
                        if href and "fn_downFile" in href:
                             down_js = href
                        else:
                             onclick = await down_btn.get_attribute('onclick')
                             if onclick and "fn_downFile" in onclick:
                                 down_js = onclick
                    
                    extracted_items.append({
                        'title': title,
                        'view_href': view_href,
                        'date': date_text,
                        'down_js': down_js
                    })
                except Exception as e:
                    print(f"[DEBUG] Extraction Error: {e}")
                    continue
            
            logger.info(f"   â†’ ì¶”ì¶œëœ ë°ì´í„° {len(extracted_items)}ê±´ ì²˜ë¦¬ ì‹œì‘")

            for data in extracted_items:
                try:
                    title = data['title']
                    date_text = data['date']
                    view_href = data['view_href']
                    down_js = data['down_js']
                    full_view_url = urljoin(self.base_url, view_href)
                    
                    # ë‚ ì§œ ì²´í¬
                    if date_text != "0000-00-00" and not self.is_in_period(date_text):
                        if date_text < str(self.start_date):
                             pass
                        continue
                        
                    # PDF URL ìƒì„± (ëª©ë¡ì—ì„œ)
                    pdf_url = "N/A"
                    if down_js:
                        pdf_url = self._extract_pdf_url_from_js(down_js)
                    
                    # (B) ìƒì„¸ í˜ì´ì§€ ì§„ì… (PDFê°€ ì—†ê±°ë‚˜ ë‚ ì§œê°€ ì—†ì„ ë•Œ)
                    # ëª©ë¡ ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•´ ìƒˆ íƒ­ ì‚¬ìš©
                    if pdf_url == "N/A" or date_text == "0000-00-00":
                        try:
                            # ìƒˆ íƒ­ ì—´ê¸°
                            new_page = await page.context.new_page()
                            await new_page.goto(full_view_url, wait_until='networkidle')
                            await new_page.wait_for_timeout(500) # ì•ˆì •í™” ëŒ€ê¸°
                            
                            # 1) ë‚ ì§œ ì¬í™•ì¸ (ìƒì„¸)
                            if date_text == "0000-00-00":
                                # dl > dd íŒ¨í„´ì´ ë§ìŒ
                                detail_date_elem = await new_page.query_selector('.viewDate, .date, .regDate, dl dd, .boardViewInfo dd')
                                if detail_date_elem:
                                     # ë°”ë¡œ í…ìŠ¤íŠ¸ê°€ ë‚ ì§œì¼ ìˆ˜ë„ ìˆê³ , "ë“±ë¡ì¼ : 2025..." í˜•ì‹ì¼ ìˆ˜ë„ ìˆìŒ
                                     txt = (await detail_date_elem.text_content()).strip()
                                     m = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', txt)
                                     if m:
                                         date_text = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
                                         
                                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
                                if date_text == "0000-00-00":
                                     full_txt = await new_page.content()
                                     m_all = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', full_txt)
                                     if m_all:
                                         date_text = f"{m_all.group(1)}-{m_all.group(2)}-{m_all.group(3)}"

                            # 2) PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ìƒì„¸)
                            if pdf_url == "N/A":
                                # ìƒì„¸ í˜ì´ì§€ì—ëŠ” ë³´í†µ fn_downFileì´ ìˆìŒ
                                detail_down_btn = await new_page.query_selector('a[href*="fn_downFile"], a[onclick*="fn_downFile"], button[onclick*="fn_downFile"]')
                                if detail_down_btn:
                                    href = await detail_down_btn.get_attribute('href')
                                    onclick = await detail_down_btn.get_attribute('onclick')
                                    
                                    js_code = href if (href and "fn_downFile" in href) else onclick
                                    if js_code:
                                        pdf_url = self._extract_pdf_url_from_js(js_code)
                            
                        except Exception as e:
                            logger.warning(f"      ìƒì„¸ í˜ì´ì§€ ì—ëŸ¬: {e}")
                        finally:
                            if 'new_page' in locals():
                                await new_page.close()
                    
                    # ìµœì¢… ì €ì¥
                    if pdf_url != "N/A":
                        if date_text == "0000-00-00": date_text = "1900-01-01"
                        
                        logger.info(f"      âœ… ìˆ˜ì§‘: {title[:20]}... ({date_text})")
                        self.save_result(title, date_text, pdf_url, full_view_url)
                        collected_count += 1
                        count_in_page += 1
                    else:
                        logger.debug(f"      PDF ì—†ìŒ (ìƒì„¸ í™•ì¸ í›„): {title}")
                        
                except Exception as e:
                    logger.warning(f"      Process Error: {e}")
            
            # í˜ì´ì§€ë„¤ì´ì…˜
            # javascript:fn_linkPage(2) í˜•ì‹
            # ë³´í†µ class="paging" ë˜ëŠ” .pagination
            next_page = current_page + 1
            next_btn = await page.query_selector(f'a[href*="fn_linkPage({next_page})"]')
            
            if next_btn:
                logger.info("   â–¶ ë‹¤ìŒ í˜ì´ì§€ í´ë¦­")
                 # JS í˜¸ì¶œì´ë¯€ë¡œ click
                await next_btn.click()
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(1000)
                current_page += 1
            else:
                # 10í˜ì´ì§€ ë‹¨ìœ„ ì´ë™ ì²˜ë¦¬ (Next >)
                # ì˜ˆ: fn_linkPage(11) ...
                # ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ ìˆ«ì ë²„íŠ¼ ì—†ìœ¼ë©´ ì¢…ë£Œ
                logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€ (ë” ì´ìƒ ìˆ«ì ë²„íŠ¼ ì—†ìŒ)")
                break

        return collected_count
        
    def _extract_pdf_url_from_js(self, js_str):
        # javascript: fn_downFile('FILE_000000002001509','0')
        m = re.search(r"fn_downFile\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)", js_str)
        if m:
            atch_file_id = m.group(1)
            file_sn = m.group(2)
            
            # ì‚¬ìš©ì ë¡œê·¸ì— ë”°ë¥´ë©´ fileSn=1 ë¡œ ìš”ì²­ë¨.
            # ì¸ìê°€ '0'ì¼ ë•Œ '1'ë¡œ ë³€í™˜í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”.
            # ë³´í†µ Java ê¸°ë°˜ ê³µí†µ ì»´í¬ë„ŒíŠ¸ì—ì„œ 0-index vs 1-index ì°¨ì´.
            # ì¼ë‹¨ ì•ˆì „í•˜ê²Œ ì¸ìê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, ì´ìŠˆ ë°œìƒ ì‹œ ìˆ˜ì •.
            if file_sn == '0':
                file_sn = '1' # ë¡œê·¸ ê¸°ë°˜ ì¶”ë¡ : 0 -> 1 ë³€í™˜ ì‹œë„
            
            return f"https://www.kbfg.com/kbresearch/cmm/fms/FileDown.do?atchFileId={atch_file_id}&fileSn={file_sn}"
        return "N/A"

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 720}
            )
            page = await context.new_page()
            
            logger.info(f"ğŸš€ [KBê¸ˆìœµì§€ì£¼] ìˆ˜ì§‘ ì‹œì‘ ({self.start_date} ~ {self.end_date})")
            try:
                await self._scrape_board(page, self.target_url)
            except Exception as e:
                logger.error(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                await browser.close()

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    import sys
    
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[KBê¸ˆìœµì§€ì£¼ ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [Default: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            end_in = input("ì¢…ë£Œì¼ (YYYY-MM-DD) [Default: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except:
            sys.exit(0)

    scraper = KBScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
    
    if scraper.results:
        import json
        out_dir = os.path.join(cur_dir, "output")
        os.makedirs(out_dir, exist_ok=True)
        fpath = os.path.join(out_dir, f"kb_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(fpath, "w", encoding="utf-8") as f:
            json.dump(scraper.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {fpath}")
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
