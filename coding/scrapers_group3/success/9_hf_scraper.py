#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì£¼íƒê¸ˆìœµì—°êµ¬ì› (HF) ìŠ¤í¬ë˜í¼ - ìµœì¢… ìˆ˜ì •ë³¸
- URL: https://researcher.hf.go.kr/researcher/sub02/sub02_05.do
- êµ¬ì¡°: div.research-area ëª©ë¡ -> ìƒì„¸(mode=view) -> ë‹¤ìš´ë¡œë“œ(a.pdf)
- ë‚ ì§œ: .info02 í…ìŠ¤íŠ¸ íŒŒì‹±
"""

import sys
import os
import asyncio
import logging
import csv
import json
import re
from datetime import datetime
from urllib.parse import urljoin

from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("HF")

class HFScraper:
    def __init__(self, start_date=None, end_date=None):
        self.base_url = "https://researcher.hf.go.kr/researcher/sub02/sub02_05.do"
        self.list_url = "https://researcher.hf.go.kr/researcher/sub02/sub02_05.do"
        
        self.start_date = self._parse_date(start_date)
        self.end_date = self._parse_date(end_date)
        
        self.results = []
        self.output_dir = os.path.join(os.path.dirname(__file__), "output")
        # ìƒìœ„ output í´ë” ì°¾ê¸°
        if not os.path.exists(self.output_dir):
             self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "output"))
             if not os.path.exists(self.output_dir): # í•œ ë‹¨ê³„ ë” ìœ„
                 self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "output"))
                 
        os.makedirs(self.output_dir, exist_ok=True)

    def _parse_date(self, date_str):
        if not date_str: return None
        try: return datetime.strptime(date_str, "%Y-%m-%d")
        except: return None

    def _is_in_period(self, date_str):
        if not date_str: return False
        try:
            date_str = date_str.strip().replace('.', '-').replace('/', '-')
            match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
            if match:
                current_date = datetime.strptime(match.group(0), "%Y-%m-%d")
            else:
                return False
            if self.start_date and current_date < self.start_date: return False
            if self.end_date and current_date > self.end_date: return False
            return True
        except: return False

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            # ì²« ì ‘ì†
            current_url = self.list_url
            logger.info(f"ğŸŒ ì ‘ì† ì¤‘: {current_url}")
            await page.goto(current_url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(2000)

            max_pages = 20
            current_page = 1
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ë£¨í”„
            while current_page <= max_pages:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page} - ëª©ë¡ ë¶„ì„ ì¤‘...")
                
                # HTML êµ¬ì¡° ë¶„ì„ ê²°ê³¼: div.research-area
                rows = await page.query_selector_all('div.research-area')
                
                if not rows:
                    logger.warning("âŒ ê²Œì‹œë¬¼ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # ë””ë²„ê¹…ìš© ë¤í”„
                    await page.screenshot(path="debug_hf_final.png")
                    break
                
                targets = []
                logger.info(f"   ğŸ” ê²Œì‹œë¬¼ {len(rows)}ê°œ ë°œê²¬")
                
                for row in rows:
                    # ì œëª©: h4 a
                    title_elem = await row.query_selector('h4 a')
                    if not title_elem: continue
                    
                    title_text = (await title_elem.text_content()).strip()
                    detail_href = await title_elem.get_attribute('href')
                    # href="?mode=view..." -> base_url + href
                    if detail_href:
                         detail_url = urljoin(self.base_url, detail_href)
                    else:
                         continue
                    
                    # ë‚ ì§œ: .info02 í…ìŠ¤íŠ¸ì—ì„œ íŒŒì‹±
                    date_text = "0000-00-00"
                    info_elem = await row.query_selector('.info02')
                    if info_elem:
                        info_text = await info_elem.text_content()
                        # 2025-12-30 íŒ¨í„´ ì°¾ê¸°
                        match_date = re.search(r'\d{4}-\d{2}-\d{2}', info_text)
                        if match_date:
                            date_text = match_date.group(0)
                    
                    if not self._is_in_period(date_text):
                        if date_text != "0000-00-00": 
                             continue
                        # ë‚ ì§œ ëª»ì°¾ì•˜ìœ¼ë©´ ì¼ë‹¨ ìƒì„¸ ê°€ì„œ í™•ì¸
                    
                    targets.append({
                        'title': title_text,
                        'date': date_text,
                        'url': detail_url
                    })

                # ìƒì„¸ í˜ì´ì§€ ìˆœíšŒ
                if targets:
                    logger.info(f"   ğŸ“‹ {len(targets)}ê°œì˜ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì‹œì‘...")
                    for t in targets:
                        try:
                            # íƒ­ì„ ì“°ì§€ ì•Šê³  gotoë¡œ ì´ë™
                            await page.goto(t['url'], wait_until='domcontentloaded')
                            
                            # PDF ë§í¬ ì°¾ê¸° (ì‚¬ìš©ì ì •ë³´: a.pdf href="?mode=download...")
                            pdf_url = "N/A"
                            download_link = await page.query_selector('a.pdf')
                            if not download_link:
                                download_link = await page.query_selector('a[href*="mode=download"]')
                            
                            if download_link:
                                href = await download_link.get_attribute('href')
                                if href:
                                    # href=?mode=download... -> list_url + href(query)
                                    pdf_url = urljoin(self.list_url, href)
                            
                            # ìƒì„¸ì—ì„œ ë‚ ì§œ ë‹¤ì‹œ í™•ì¸
                            if t['date'] == "0000-00-00":
                                page_text = await page.content()
                                match_date = re.search(r'\d{4}-\d{2}-\d{2}', page_text)
                                if match_date:
                                    t['date'] = match_date.group(0)
                                    if not self._is_in_period(t['date']):
                                        continue

                            if pdf_url != "N/A":
                                logger.info(f"      âœ… ìˆ˜ì§‘: {t['title'][:20]}... ({t['date']})")
                                self.results.append({
                                    'source': 'HF',
                                    'title': t['title'],
                                    'date': t['date'],
                                    'pdf_url': pdf_url,
                                    'collected_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                })
                        except Exception as e:
                            logger.error(f"      âš ï¸ ìƒì„¸ í˜ì´ì§€ ì—ëŸ¬: {e}")
                
                # ëª©ë¡ í˜ì´ì§€ ë³µê·€ (í˜ì´ì§€ë„¤ì´ì…˜ ì´ë™ì„ ìœ„í•´)
                # offset ê³„ì‚°: current_page * 10 (1í˜ì´ì§€=0~9, 2í˜ì´ì§€=10~19ë¡œ ê°€ì • ì‹œ)
                # ê·¼ë° 1í˜ì´ì§€(offset 0)ê°€ ê¸°ë³¸. 2í˜ì´ì§€ëŠ” offset 10ì¼ ê²ƒì„.
                next_offset = current_page * 10
                next_page_url = f"{self.base_url}?article.offset={next_offset}&articleLimit=10"
                
                logger.info(f"   ğŸ”„ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™(URL Load): {next_page_url}")
                await page.goto(next_page_url, wait_until='domcontentloaded')
                
                # í˜ì´ì§€ ì´ë™ ì„±ê³µ ì—¬ë¶€ í™•ì¸ (ê²Œì‹œë¬¼ ìˆëŠ”ê°€?)
                check_rows = await page.query_selector_all('div.research-area')
                if not check_rows:
                    logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ (ê²Œì‹œë¬¼ ì—†ìŒ)")
                    break
                
                current_page += 1

            await browser.close()
            self.save_files()

    def save_files(self):
        if not self.results:
            logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = os.path.join(self.output_dir, f"hf_results_{timestamp}.csv")
        json_path = os.path.join(self.output_dir, f"hf_results_{timestamp}.json")
        
        # CSV ì €ì¥
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=self.results[0].keys())
            writer.writeheader()
            writer.writerows(self.results)
        
        # JSON ì €ì¥
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)
            
        logger.info(f"ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: {json_path}")

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n" + "="*50)
        print("ì£¼íƒê¸ˆìœµì—°êµ¬ì› (HF) ìŠ¤í¬ë˜í¼")
        print("="*50)
        try:
            today = datetime.now().strftime("%Y-%m-%d")
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            
            end_in = input(f"ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: {today}]: ").strip()
            end_date = end_in if end_in else today
        except KeyboardInterrupt:
            sys.exit(0)

    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    scraper = HFScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
