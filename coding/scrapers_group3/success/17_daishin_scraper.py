#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëŒ€ì‹ ì¦ê¶Œ ìŠ¤í¬ë˜í¼
- URL: https://money2.daishin.com/E5/ResearchCenter/Work/DW_ResearchReits.aspx...
- êµ¬ì¡°: ASP.NET WebForms
- íŠ¹ì§•: ì²¨ë¶€íŒŒì¼ ë²„íŠ¼ì´ ì´ë¯¸ì§€(btn_file3.gif)ì´ë©°, hrefì— ë‹¤ìš´ë¡œë“œ ë§í¬(filedownload.aspx?rowid=...)ê°€ ì¡´ì¬í•¨.
"""

import sys
import os
import asyncio
import logging
import re
from datetime import datetime
from urllib.parse import urljoin
from playwright.async_api import async_playwright

# ìƒìœ„ í´ë”(success)ì˜ ë¶€ëª¨(scrapers_group3)ì—ì„œ base.pyë¥¼ ì°¾ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
cur_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(cur_dir) # scrapers_group3
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

try:
    from success.base import AsyncBaseScraper
except ImportError:
    # run_standalone ë“±ì˜ ìƒí™© ê³ ë ¤
    sys.path.append(cur_dir)
    try:
        from base import AsyncBaseScraper
    except ImportError:
        print("âŒ base.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("DaishinScraper")

class DaishinScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        super().__init__(start_date, end_date, site_name="ëŒ€ì‹ ì¦ê¶Œ")
        # ë¦¬ì¸ /ë¶€ë™ì‚° ì„¹ì…˜ URL
        self.target_url = "https://money2.daishin.com/E5/ResearchCenter/Work/DW_ResearchReits.aspx?m=10904&p=11112&v=11661"
        self.base_url = "https://money2.daishin.com"

    async def _scrape_board(self, page, url):
        await page.goto(url, wait_until='networkidle', timeout=30000)
        await page.wait_for_timeout(2000)
        
        collected_count = 0
        current_page = 1
        max_pages = 10 
        
        while current_page <= max_pages:
            logger.info(f"   ğŸ“„ í˜ì´ì§€ {current_page} ì½ëŠ” ì¤‘...")
            
            # 1. ì²¨ë¶€íŒŒì¼ ë²„íŠ¼ ì§ì ‘ íƒìƒ‰ (ê°€ì¥ í™•ì‹¤í•œ ì§€í‘œ)
            file_btns = await page.query_selector_all('img[src*="btn_file"]')
            
            if not file_btns:
                logger.warning("   âš ï¸ ì²¨ë¶€íŒŒì¼ ë²„íŠ¼(btn_file)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ë””ë²„ê¹…: HTML êµ¬ì¡° í™•ì¸
                # html = await page.content()
                # logger.info(f"HTML Dump (First 1000 chars): {html[:1000]}")
                break
                
            logger.info(f"   â†’ ì•„ì´í…œ(íŒŒì¼ë²„íŠ¼) {len(file_btns)}ê°œ ë°œê²¬")
            
            count_in_page = 0
            
            for btn_img in file_btns:
                try:
                    # 1. ì œëª©, ë‚ ì§œ ì¶”ì¶œ (img íƒœê·¸ì˜ alt ì†ì„± í™œìš©)
                    alt_text = await btn_img.get_attribute('alt')
                    # ì˜ˆ: "[ëŒ€ì‹ ì¦ê¶Œ ë‚˜ë¯¸ì„ ] ì›”ê°„ ì¼ë³¸ ë¶€ë™ì‚° (2026ë…„ 01ì›”)  ë‹¤ìš´ë¡œë“œ"
                    
                    if not alt_text:
                        logger.warning("      alt ì†ì„± ì—†ìŒ")
                        continue

                    # ë‚ ì§œ íŒŒì‹± (alt ê¸°ì¤€)
                    date_text = "0000-00-00"
                    # (\d{4})ë…„ (\d{2})ì›”
                    m_date = re.search(r'(\d{4})ë…„\s*(\d{2})ì›”', alt_text)
                    if m_date:
                        date_text = f"{m_date.group(1)}-{m_date.group(2)}-01" # 1ì¼ë¡œ ê°€ì •
                    else:
                        # ë‹¤ë¥¸ ë‚ ì§œ íŒ¨í„´ ì‹œë„ (YYYY.MM.DD)
                        m_date2 = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', alt_text)
                        if m_date2:
                            date_text = f"{m_date2.group(1)}-{m_date2.group(2)}-{m_date2.group(3)}"

                    # ì œëª© íŒŒì‹±
                    # [...] ... ë‹¤ìš´ë¡œë“œ íŒ¨í„´ ì œê±°
                    title = alt_text
                    if "ë‹¤ìš´ë¡œë“œ" in title:
                        title = title.replace("ë‹¤ìš´ë¡œë“œ", "")
                    title = title.strip()
                    
                    # 2. ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ (ì´ë¯¸ì§€ì˜ ë¶€ëª¨ a íƒœê·¸)
                    pdf_url = "N/A"
                    
                    # ë¶€ëª¨ a íƒœê·¸ ì°¾ê¸°
                    parent_link_handle = await btn_img.evaluate_handle('el => el.closest("a")')
                    parent_link = parent_link_handle.as_element() if parent_link_handle else None
                    
                    if parent_link:
                        href = await parent_link.get_attribute('href')
                        if href:
                            pdf_url = urljoin(self.base_url, href)
                    else:
                        # í˜¹ì‹œ ë¶€ëª¨ê°€ ì•„ë‹ˆë¼ ê·¼ì²˜ì— ìˆëŠ” ê²½ìš° (í˜•ì œ)
                        # ì´ ë¶€ë¶„ì€ êµ¬ì¡°ë¥¼ ëª¨ë¥´ë©´ ì–´ë ¤ìš°ë‚˜, ë³´í†µ a > img êµ¬ì¡°ì„.
                        # ì‚¬ìš©ì ì •ë³´: ë²„íŠ¼í´ë¦­ì‹œ ë‚˜ì˜¤ëŠ” ìš”ì†Œ_<img> ...
                        # í´ë¦­í•´ì„œ ë‹¤ìš´ë¡œë“œëœë‹¤ë©´ onclickì´ë‚˜ ë¶€ëª¨ aê°€ ìˆì–´ì•¼ í•¨.
                        pass

                    # 3. ê¸°ê°„ ì²´í¬ & ì €ì¥
                    if not self.is_in_period(date_text):
                        # ë„ˆë¬´ ê³¼ê±° ë°ì´í„°ë©´ ìŠ¤í‚µ
                        if date_text != "0000-00-00" and date_text < str(self.start_date):
                             pass
                        continue
                    
                    if pdf_url != "N/A":
                        logger.info(f"      âœ… ìˆ˜ì§‘: {title[:20]}... ({date_text})")
                        self.save_result(title, date_text, pdf_url, page.url)
                        collected_count += 1
                        count_in_page += 1
                    else:
                        logger.warning(f"      ë§í¬ ì°¾ê¸° ì‹¤íŒ¨: {title}")
                        
                except Exception as e:
                    logger.warning(f"      Item Error: {e}")
                    continue

            # í˜ì´ì§€ë„¤ì´ì…˜
            # <div class="paging"> ... <a ...>Next</a>
            # ASP.NETì€ ë³´í†µ 1, 2, 3... ìˆ«ì ë²„íŠ¼ê³¼ ì´ì „/ë‹¤ìŒ í™”ì‚´í‘œê°€ ìˆìŒ.
            # ë‹¤ìŒ í˜ì´ì§€ ìˆ«ìë¥¼ ì°¾ê±°ë‚˜ 'ë‹¤ìŒ' ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì•¼ í•¨.
            
            # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°
            next_page_num = current_page + 1
            
            # 1. ìˆ«ì ë²„íŠ¼ í´ë¦­ ì‹œë„ (1 2 [3] 4 5 ...)
            # <a>2</a>
            next_btn = await page.query_selector(f'.paging a:text("{next_page_num}")')
            
            # 2. ì—†ìœ¼ë©´ 'ë‹¤ìŒ' ì´ë¯¸ì§€/ë²„íŠ¼ í´ë¦­ (10í˜ì´ì§€ ë‹¨ìœ„ ë„˜ì–´ê°ˆë•Œ)
            if not next_btn:
                # alt="ë‹¤ìŒ" ë˜ëŠ” class="btn_next" ë“±
                next_btn = await page.query_selector('.paging a[href*="Next"], .paging .next, img[alt="ë‹¤ìŒ"]')

            if next_btn:
                logger.info("   â–¶ ë‹¤ìŒ í˜ì´ì§€ í´ë¦­")
                await next_btn.click()
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(2000) # ASP.NET postback ëŒ€ê¸°
                current_page += 1
            else:
                logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€")
                break
                
        return collected_count

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            
            logger.info(f"ğŸš€ [ëŒ€ì‹ ì¦ê¶Œ] ìˆ˜ì§‘ ì‹œì‘ ({self.start_date} ~ {self.end_date})")
            try:
                await self._scrape_board(page, self.target_url)
            except Exception as e:
                logger.error(f"âŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                await browser.close()

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    import sys
    
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[ëŒ€ì‹ ì¦ê¶Œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [Default: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            
            end_in = input("ì¢…ë£Œì¼ (YYYY-MM-DD) [Default: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except:
            sys.exit(0)

    scraper = DaishinScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
    
    # ê²°ê³¼ ì €ì¥ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
    if scraper.results:
        import json
        out_dir = os.path.join(cur_dir, "output")
        os.makedirs(out_dir, exist_ok=True)
        fpath = os.path.join(out_dir, f"daishin_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(fpath, "w", encoding="utf-8") as f:
            json.dump(scraper.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {fpath}")
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
