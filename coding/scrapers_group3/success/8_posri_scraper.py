#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í¬ìŠ¤ì½”ê²½ì˜ì—°êµ¬ì› (POSRI) ìŠ¤í¬ë˜í¼ - ìµœì¢… ìˆ˜ì •ë³¸
- URL: https://www.posri.re.kr/kor/bbs/report_list.do...
- ëŒ€ìƒ: ì—°êµ¬ë³´ê³ ì„œ (ì´ìŠˆë¦¬í¬íŠ¸)
- êµ¬ì¡°: div.lst-customer-type1 > div.inner > div.item
- ìˆ˜ì§‘: ì œëª©, ë‚ ì§œ, PDF ë‹¤ìš´ë¡œë“œ ë§í¬ (a.btn-txt-down ë˜ëŠ” a[href*='download.do'])
"""

import sys
import os
import asyncio
import logging
import csv
import json
import re
from datetime import datetime
from urllib.parse import urljoin

from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("POSRI")

class POSRIScraper:
    def __init__(self, start_date=None, end_date=None):
        self.base_url = "https://www.posri.re.kr"
        self.target_url = "https://www.posri.re.kr/kor/bbs/report_list.do?mmcd=2402221432440016120&cate=2403071010350015910"
        
        self.start_date = self._parse_date(start_date)
        self.end_date = self._parse_date(end_date)
        
        self.results = []
        self.output_dir = os.path.join(os.path.dirname(__file__), "output")
        # ìƒìœ„ output í´ë” ì°¾ê¸°
        if not os.path.exists(self.output_dir):
             self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "output"))
             if not os.path.exists(self.output_dir): # í•œ ë‹¨ê³„ ë” ìœ„
                 self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "output"))
                 
        os.makedirs(self.output_dir, exist_ok=True)

    def _parse_date(self, date_str):
        if not date_str: return None
        try: return datetime.strptime(date_str, "%Y-%m-%d")
        except: return None

    def _is_in_period(self, date_str):
        if not date_str: return False
        try:
            date_str = date_str.strip().replace('.', '-').replace('/', '-')
            match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
            if match:
                current_date = datetime.strptime(match.group(0), "%Y-%m-%d")
            else:
                return False
            if self.start_date and current_date < self.start_date: return False
            if self.end_date and current_date > self.end_date: return False
            return True
        except: return False

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            logger.info(f"ğŸŒ ì ‘ì† ì¤‘: {self.target_url}")
            await page.goto(self.target_url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(2000)

            max_pages = 20
            current_page = 1
            
            while current_page <= max_pages:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page} - ëª©ë¡ ë¶„ì„ ì¤‘...")
                
                # HTML êµ¬ì¡° ë¶„ì„ì— ê¸°ë°˜í•œ ì •í™•í•œ ì„ íƒì
                items = await page.query_selector_all('.lst-customer-type1 .item, div.item.hv_type.sz2')
                
                if not items:
                    logger.warning("âŒ ê²Œì‹œë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # ë””ë²„ê¹…ìš© ë¤í”„
                    await page.screenshot(path="debug_posri_final.png")
                    break
                
                logger.info(f"   ï¿½ ê²Œì‹œë¬¼ {len(items)}ê°œ ë°œê²¬")
                page_collected_count = 0
                
                for item in items:
                    # ì œëª© (.h_1 a ë˜ëŠ” .h_1)
                    title_elem = await item.query_selector('.h_1 a')
                    if not title_elem:
                        title_elem = await item.query_selector('.h_1')
                        
                    if not title_elem: continue
                    title_text = (await title_elem.text_content()).strip()

                    # ë‚ ì§œ (.info span ì²«ë²ˆì§¸)
                    date_text = "0000-00-00"
                    info_spans = await item.query_selector_all('.info span')
                    if info_spans:
                        # ì²«ë²ˆì§¸ spanì´ ë³´í†µ ë‚ ì§œ (2026.01.21)
                        # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ - ë˜ëŠ” . ì´ ìˆëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸°
                        for span in info_spans:
                            txt = (await span.text_content()).strip()
                            if re.search(r'\d{4}[\.-]\d{2}[\.-]\d{2}', txt):
                                date_text = txt
                                break

                    # ë‚ ì§œ í¬ë§· í†µì¼ (YYYY-MM-DD)
                    date_text = date_text.replace('.', '-')
                    match_date = re.search(r'\d{4}-\d{2}-\d{2}', date_text)
                    if match_date:
                        date_text = match_date.group(0)
                        
                    if not self._is_in_period(date_text): continue
                    
                    # ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ
                    # ì‚¬ìš©ì ìš”ì²­: <a href="/download.do..." class="btn-txt-down">
                    pdf_url = "N/A"
                    download_link = await item.query_selector('a[href*="download.do"]')
                    
                    if download_link:
                        href = await download_link.get_attribute('href')
                        if href:
                            # href="/download.do..." -> ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
                            pdf_url = urljoin(self.base_url, href)
                    
                    # ìˆ˜ì§‘ í™•ì¸
                    if pdf_url != "N/A":
                        logger.info(f"      âœ… ìˆ˜ì§‘: {title_text[:20]}... ({date_text})")
                        self.results.append({
                            'source': 'POSRI',
                            'title': title_text,
                            'date': date_text,
                            'pdf_url': pdf_url,
                            'collected_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        })
                        page_collected_count += 1
                
                # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ íŒë‹¨
                if page_collected_count == 0 and len(items) > 0:
                     logger.info("   â¹ï¸ ë‚ ì§œ ë²”ìœ„ ì´ˆê³¼(ë˜ëŠ” í•´ë‹¹ ì—†ìŒ)ë¡œ ì¢…ë£Œ ì²´í¬")
                     # í¬ìŠ¤ì½”ëŠ” ìµœì‹ ìˆœ ë‚˜ì—´ì´ë¯€ë¡œ, ì—¬ê¸°ì„œ ë©ˆì¶°ë„ ë ì§€ íŒë‹¨ì€ ìœ ì € ëª«ì´ë‚˜ ì¼ë‹¨ break
                     if self.results: # ì´ë¯¸ ìˆ˜ì§‘ëœê²Œ ìˆë‹¤ë©´ ë‚ ì§œ ì§€ë‚œê±°ë‹ˆ ì¢…ë£Œ
                         break
                
                # í˜ì´ì§€ë„¤ì´ì…˜
                try:
                    next_page = current_page + 1
                    # onclick="fn_link_page(2)"
                    next_btn = await page.query_selector(f'a[onclick*="fn_link_page({next_page})"]')
                    
                    # ì—†ìœ¼ë©´ > ë²„íŠ¼
                    if not next_btn:
                        next_btn = await page.query_selector('.paging .next, a.btn-next')

                    if next_btn:
                        await next_btn.click()
                        await page.wait_for_timeout(2000)
                        current_page += 1
                    else:
                        logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                        break
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ ì´ë™ ì¤‘ ì—ëŸ¬/ì¢…ë£Œ: {e}")
                    break

            await browser.close()
            self.save_files()

    def save_files(self):
        if not self.results:
            logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = os.path.join(self.output_dir, f"posri_results_{timestamp}.csv")
        json_path = os.path.join(self.output_dir, f"posri_results_{timestamp}.json")
        
        # CSV ì €ì¥
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=self.results[0].keys())
            writer.writeheader()
            writer.writerows(self.results)
        
        # JSON ì €ì¥
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)
            
        logger.info(f"ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: {json_path}")

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n" + "="*50)
        print("í¬ìŠ¤ì½”ê²½ì˜ì—°êµ¬ì› (POSRI) ìŠ¤í¬ë˜í¼")
        print("="*50)
        try:
            today = datetime.now().strftime("%Y-%m-%d")
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            
            end_in = input(f"ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: {today}]: ").strip()
            end_date = end_in if end_in else today
        except KeyboardInterrupt:
            sys.exit(0)

    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    scraper = POSRIScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
