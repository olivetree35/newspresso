import os
import sys
import asyncio
import re
import json
from urllib.parse import urljoin
from datetime import datetime
from playwright.async_api import async_playwright

# ìƒìœ„ í´ë”(base.pyê°€ ìˆëŠ” ê³³)ë¥¼ sys.pathì— ì¶”ê°€
cur_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(cur_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from base import AsyncBaseScraper

class CushmanScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        super().__init__(start_date, end_date, "Cushman & Wakefield")
        self.base_url = "https://www.cushmanwakefield.com"
        # ì¿ ì‹œë¨¼ í•œêµ­ ë¦¬ì„œì¹˜ í˜ì´ì§€ (ì¸ì‚¬ì´íŠ¸)
        self.target_url = "https://www.cushmanwakefield.com/ko-kr/south-korea/insights?q=&sort=date%20descending"

    async def scrape(self):
        collected_count = 0  # [ìˆ˜ì •] ì´ˆê¸°í™” ìœ„ì¹˜ ìƒí–¥ (UnboundLocalError ë°©ì§€)
        
        async with async_playwright() as p:
            # ë´‡ íƒì§€ ìš°íšŒë¥¼ ìœ„í•œ ë¸Œë¼ìš°ì € ì„¤ì •
            browser = await p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            # ì¼ë°˜ì ì¸ ìœ ì € ì—ì´ì „íŠ¸ ì‚¬ìš©
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                device_scale_factor=1,
            )
            
            # ì€ì‹  ìŠ¤í¬ë¦½íŠ¸ (navigator.webdriver ê°ì¶¤)
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)

            page = await context.new_page()
            
            print(f"ğŸš€ [ì¿ ì‹œë¨¼] ìˆ˜ì§‘ ì‹œì‘ ({self.start_date} ~ {self.end_date})")
            
            try:
                # íƒ€ì„ì•„ì›ƒ 60ì´ˆ
                await page.goto(self.target_url, wait_until='networkidle', timeout=60000)
                
                # 1. ì¿ í‚¤ ë™ì˜ íŒì—… ì²˜ë¦¬ (OneTrust)
                try:
                    accept_btn = await page.wait_for_selector('#onetrust-accept-btn-handler', state='visible', timeout=5000)
                    if accept_btn:
                        print("   ğŸª ì¿ í‚¤ ë™ì˜ íŒì—… ê°ì§€. 'ìˆ˜ë½' í´ë¦­.")
                        await accept_btn.click()
                        await page.wait_for_timeout(2000)
                except:
                    pass

                # ê°•ì œ ëŒ€ê¸° (ì‚¬ì´íŠ¸ ë¡œë”©)
                await page.wait_for_timeout(3000)
                
                # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ìˆ˜ì§‘ì„ ìœ„í•œ ë§í¬ ì €ì¥ì†Œ
                post_links = []
                
                # [ìˆ˜ì •] ë©”ì¸ í”„ë ˆì„ íƒìƒ‰ ë° ì²˜ë¦¬ ë¡œì§ í™•ë³´
                # CoveoResultLink ì°¾ê¸°
                try:
                    await page.wait_for_selector('.CoveoResultLink', timeout=10000)
                except:
                    print("   âš ï¸ Selector '.CoveoResultLink' íƒ€ì„ì•„ì›ƒ. ëŒ€ì²´ ë°©ë²• ì‹œë„.")

                # 1. ë©”ì¸ í”„ë ˆì„ .CoveoResultLink í™•ì¸
                main_items = await page.query_selector_all('.CoveoResultLink')
                if main_items:
                    print(f"   ğŸ‰ ë©”ì¸ í”„ë ˆì„ì—ì„œ {len(main_items)}ê°œ ë°œê²¬!")
                    for item in main_items:
                        title = (await item.text_content()).strip()
                        href = await item.get_attribute('href')
                        if href:
                            full_url = urljoin(self.base_url, href)
                            post_links.append({'title': title, 'url': full_url})

                # 2. iframe íƒìƒ‰ (ë©”ì¸ì— ì—†ìœ¼ë©´)
                if not post_links:
                    print("   â„¹ï¸ ë©”ì¸ í”„ë ˆì„ì— ê²°ê³¼ ì—†ìŒ. iframe íƒìƒ‰...")
                    for frame in page.frames:
                        f_items = await frame.query_selector_all('.CoveoResultLink')
                        if f_items:
                            print(f"   ğŸ‰ iframe({frame.name or frame.url[-20:]})ì—ì„œ {len(f_items)}ê°œ ë°œê²¬!")
                            for item in f_items:
                                title = (await item.text_content()).strip()
                                href = await item.get_attribute('href')
                                if href:
                                    full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                                    post_links.append({'title': title, 'url': full_url})
                            break # í•˜ë‚˜ ì°¾ìœ¼ë©´ ì¤‘ë‹¨

                # 3. Brute Force (ìµœí›„ì˜ ìˆ˜ë‹¨: ëª¨ë“  a íƒœê·¸)
                if not post_links:
                    print("   ğŸ”¥ [Fallback] ëª¨ë“  a íƒœê·¸ ì „ìˆ˜ ì¡°ì‚¬ (Brute Force)...")
                    all_anchors = await page.query_selector_all('a')
                    print(f"   â†’ a íƒœê·¸ ì´ {len(all_anchors)}ê°œ ìŠ¤ìº”")
                    
                    keywords = ["report", "outlook", "trend", "insight", "ë³´ê³ ì„œ", "ì „ë§", "ë™í–¥", "ë§ˆì¼“", "ì‹œì¥"]
                    seen_urls = set()
                    
                    for a in all_anchors:
                        try:
                            # í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ìˆ¨ê²¨ì§„ ìš”ì†Œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ
                            txt = (await a.text_content() or "").strip().lower()
                            href = await a.get_attribute('href')
                            
                            if not href or href.startswith('#') or href.startswith('javascript'):
                                continue
                                
                            full_url = href if href.startswith('http') else urljoin(self.base_url, href)
                            
                            # í•„í„°ë§
                            if full_url in seen_urls: continue
                            
                            is_target = False
                            # ìƒì„¸ í˜ì´ì§€ URL íŒ¨í„´ í™•ì¸
                            if "/insights/" in full_url or ".pdf" in full_url:
                                is_target = True
                            # ë˜ëŠ” í…ìŠ¤íŠ¸ì— í‚¤ì›Œë“œ í¬í•¨ (ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì£¼ì˜)
                            elif len(txt) > 4 and any(k in txt for k in keywords):
                                is_target = True
                                
                            if is_target:
                                seen_urls.add(full_url)
                                post_links.append({'title': txt or "No Title", 'url': full_url})
                        except:
                            continue

                print(f"   â†’ ìµœì¢… ìˆ˜ì§‘ ëŒ€ìƒ ë§í¬: {len(post_links)}ê°œ")

                # ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš© ìƒìœ„ 30ê°œ)
                for i, post in enumerate(post_links[:30]):
                    try:
                        await self._scrape_detail(context, post['title'], post['url'])
                        collected_count += 1
                    except Exception as e:
                        print(f"      âš ï¸ ìƒì„¸ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
            
            except Exception as e:
                print(f"âŒ í° ì—ëŸ¬ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()

            # ê²°ê³¼ ë¦¬í¬íŠ¸ ë° ì €ì¥
            print(f"\nğŸ [ì¿ ì‹œë¨¼] ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {collected_count}ê±´")
            if self.results:
                # [ìˆ˜ì •] json ëª¨ë“ˆ ìƒë‹¨ import í–ˆìœ¼ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_dir = os.path.join(cur_dir, "output")
                os.makedirs(output_dir, exist_ok=True)
                filename = f"cushman_results_{timestamp}.json"
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=4)
                print(f"   ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            
            await browser.close()
            return collected_count

    async def _scrape_detail(self, context, title, url):
        page = await context.new_page()
        try:
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(1000) # ì•ˆì •í™” ëŒ€ê¸°
            
            body_text = await page.inner_text('body') or ""
            
            # ë‚ ì§œ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
            date_text = "0000-00-00"
            
            # 1. ë©”íƒ€ ë°ì´í„° ë“±ì—ì„œ ì°¾ê¸° (ì •ê·œì‹ í™•ì¥)
            # YYYY.MM.DD or YYYY-MM-DD
            m1 = re.search(r'20\d{2}[\.-]\s*\d{1,2}[\.-]\s*\d{1,2}', body_text[:3000])
            if m1:
                raw_date = m1.group(0).replace(' ', '')
                parts = re.split(r'[\.-]', raw_date)
                date_text = f"{parts[0]}-{int(parts[1]):02d}-{int(parts[2]):02d}"
            else:
                # ì˜ë¬¸ ë‚ ì§œ (May 12, 2024 ë“±)
                months = "January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Oct|Nov|Dec"
                m2 = re.search(r'(' + months + r')\.?\s+(\d{1,2}),?\s+(20\d{2})', body_text[:3000], re.IGNORECASE)
                if m2:
                    try:
                        m_str, d_str, y_str = m2.groups()
                        # ì›” ì´ë¦„(ë¬¸ìì—´)ì„ íŒŒì‹±
                        date_str = f"{m_str} {d_str} {y_str}"
                        # %B: Full month name, %b: Abbreviated month name
                        # ë‘ ì¼€ì´ìŠ¤ ëª¨ë‘ ì²˜ë¦¬ ìœ„í•´ ì‹œë„
                        try:
                            dt = datetime.strptime(date_str, "%B %d %Y")
                        except:
                            dt = datetime.strptime(date_str, "%b %d %Y")
                            
                        date_text = dt.strftime("%Y-%m-%d")
                    except:
                        pass

            # ë‚ ì§œ í•„í„°ë§
            if not self.is_in_period(date_text):
                # ë‚ ì§œê°€ íŒŒì‹±ë˜ì—ˆëŠ”ë° ë²”ìœ„ ë°–ì´ë©´ íŒ¨ìŠ¤ (0000-00-00ì€ ì¼ë‹¨ í†µê³¼)
                if date_text != "0000-00-00":
                    valid_start = str(self.start_date)
                    valid_end = str(self.end_date)
                    if not (valid_start <= date_text <= valid_end):
                        # print(f"      íŒ¨ìŠ¤ (ë‚ ì§œ ë²”ìœ„ ì´ˆê³¼): {date_text}")
                        return

            # PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ ([ìˆ˜ì •] Selector ì˜¤ë¥˜ ë°©ì§€ -> Python ë¡œì§ìœ¼ë¡œ ì²˜ë¦¬)
            pdf_url = "N/A"
            
            # ëª¨ë“  a íƒœê·¸ë¥¼ ê°€ì ¸ì™€ì„œ Python ë ˆë²¨ì—ì„œ ê²€ì‚¬
            all_links = await page.query_selector_all('a')
            for link in all_links:
                try:
                    href = await link.get_attribute('href')
                    if not href: continue
                    
                    href_lower = href.lower()
                    txt = (await link.text_content() or "").strip().lower()
                    
                    # ì¡°ê±´: hrefì— .pdf í¬í•¨ OR (class/textì— download ë“± í¬í•¨ AND hrefê°€ ìœ íš¨)
                    is_pdf = False
                    if ".pdf" in href_lower:
                        is_pdf = True
                    elif "download" in txt or "ë‹¤ìš´ë¡œë“œ" in txt:
                        is_pdf = True
                    
                    if is_pdf:
                        temp_url = urljoin(url, href)
                        # ì‹¤ì œ .pdf í™•ì¥ì í™•ì¸ (ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì¸ë° html ë§í¬ì¼ ìˆ˜ë„ ìˆìŒ)
                        if ".pdf" in temp_url.lower():
                            pdf_url = temp_url
                            break
                except:
                    continue

            if pdf_url != "N/A":
                print(f"      âœ… ìˆ˜ì§‘: {title[:15]}... ({date_text})")
                self.save_result(title, date_text, pdf_url, url)
            else:
                 # PDF ì—†ì–´ë„ ë‚ ì§œê°€ ìœ íš¨í•˜ë©´ ì €ì¥ (ë§í¬ë¼ë„ ê±´ì§€ê²Œ)
                 pass

        except Exception as e:
            # print(f"      âš ï¸ ìƒì„¸ í˜ì´ì§€ ì—ëŸ¬: {e}")
            pass 
        finally:
            await page.close()

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    import sys
    
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[Cushman & Wakefield ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            end_in = input("ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            sys.exit(0)

    scraper = CushmanScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
