
import os
import sys
import asyncio
import re
import json
from urllib.parse import urljoin
from datetime import datetime
from playwright.async_api import async_playwright

# ìƒìœ„ í´ë”(base.pyê°€ ìˆëŠ” ê³³)ë¥¼ sys.pathì— ì¶”ê°€
cur_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(cur_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from base import AsyncBaseScraper

class KyoboScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        super().__init__(start_date, end_date, "êµë³´ë¦¬ì–¼ì½”")
        self.base_url = "https://www.kyoborealco.co.kr"
        self.target_url = "https://www.kyoborealco.co.kr/insight/marketreport"

    async def scrape(self):
        collected_count = 0
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080}
            )
            
            page = await context.new_page()
            print(f"ğŸš€ [êµë³´ë¦¬ì–¼ì½”] ìˆ˜ì§‘ ì‹œì‘ ({self.start_date} ~ {self.end_date})")
            
            try:
                # 1. í˜ì´ì§€ ì ‘ì†
                await page.goto(self.target_url, wait_until='networkidle', timeout=60000)
                await page.wait_for_timeout(3000) # ë Œë”ë§ ëŒ€ê¸°
                
                # 2. ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ ì‹œë„ (tbody tr ë˜ëŠ” ê²Œì‹œíŒ í˜•íƒœ ì¶”ì •)
                # ì œê³µëœ íŒíŠ¸: <a href="/insight/files/download?fileUid=...">
                # ì „ì²´ a íƒœê·¸ ìŠ¤ìº” í›„ ë¦¬í¬íŠ¸ í•­ëª©ìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë“¤ í•„í„°ë§
                
                links = await page.query_selector_all('a[href*="/insight/files/download"]')
                print(f"   ğŸ” ë°œê²¬ëœ ë‹¤ìš´ë¡œë“œ ë§í¬ ìˆ˜: {len(links)}ê°œ")
                
                processed_urls = set()
                
                for link in links:
                    try:
                        download_href = await link.get_attribute('href')
                        full_download_url = urljoin(self.base_url, download_href)
                        
                        if full_download_url in processed_urls:
                            continue
                        
                        # ì´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ì†í•œ 'í–‰(row)' ì°¾ê¸°
                        # ì´ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ ì†í•œ 'í–‰(row)' ì°¾ê¸°
                        row = await link.evaluate_handle("el => el.closest('tr')")
                        if not row.as_element():
                            # print("   âš ï¸ tr ì—†ìŒ, li ì‹œë„...")
                            # lië§Œ ì¡ìœ¼ë©´ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ ê·¸ ë¶€ëª¨ì¸ ulì„ ì‹œë„
                            row = await link.evaluate_handle("el => el.closest('ul')")
                        
                        if not row.as_element():
                             # ê·¸ë˜ë„ ì—†ìœ¼ë©´ div.item ì‹œë„
                             row = await link.evaluate_handle("el => el.closest('div.board_list_item, div.item')")
                        
                        if not row.as_element():
                             print("   âš ï¸ ë¶€ëª¨ ìš”ì†Œ(tr/ul/div) ì°¾ê¸° ì‹¤íŒ¨")
                             continue

                            
                        title = "No Title"
                        date_text = "0000-00-00"
                        
                        # 3. ì œëª© ë° ë‚ ì§œ ì¶”ì¶œ
                        if row:
                            # JSHandle null check ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            row_text = await row.evaluate("el => el ? (el.innerText || el.textContent) : ''")
                            if not row_text or not row_text.strip():
                                # print("   âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ (Empty Text)")
                                continue
                            # print(f"Row Text: {row_text[:50]}...") # ë””ë²„ê·¸ìš©

                            
                            # ë‚ ì§œ ì¶”ì¶œ (YYYY.MM.DD)
                            date_match = re.search(r'20\d{2}[.-]\d{1,2}[.-]\d{1,2}', row_text)
                            if date_match:
                                date_str = date_match.group(0).replace('.','-')
                                date_text = date_str
                            
                            # ì œëª© ì¶”ì¶œ (ë‚ ì§œë‚˜ ë‹¤ìš´ë¡œë“œ ë“± ì œì™¸í•œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸)
                            # ë³´í†µ ì œëª©ì´ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ì¼ í™•ë¥ ì´ ë†’ìŒ, í˜¹ì€ a íƒœê·¸ í…ìŠ¤íŠ¸
                            # ì—¬ê¸°ì„œëŠ” row ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œë¥¼ ì œì™¸í•˜ê³  ì •ì œí•˜ëŠ” ë°©ì‹ ì‚¬ìš©
                            lines = row_text.split('\n')
                            for line in lines:
                                if len(line.strip()) > 5 and not re.search(r'20\d{2}[.-]', line):
                                    title = line.strip()
                                    break
                                    
                        # 4. ë‚ ì§œ í•„í„°ë§
                        if not self.is_in_period(date_text):
                            if date_text != "0000-00-00":
                                continue

                        # 5. ì €ì¥
                        if full_download_url not in processed_urls:
                            print(f"      âœ… ìˆ˜ì§‘: {title[:20]}... ({date_text})")
                            self.save_result(title, date_text, full_download_url, self.target_url)
                            processed_urls.add(full_download_url)
                            collected_count += 1
                            
                    except Exception as e:
                        print(f"      âš ï¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                        continue
                        
            except Exception as e:
                print(f"âŒ ìˆ˜ì§‘ ì¤‘ í° ì—ëŸ¬ ë°œìƒ: {e}")
            
            # ê²°ê³¼ ì €ì¥
            if self.results:
                output_dir = os.path.join(cur_dir, "output")
                os.makedirs(output_dir, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"kyobo_results_{timestamp}.json"
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=4)
                print(f"   ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            else:
                print("   âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ (Selectorë‚˜ ë¡œì§ ì ê²€ í•„ìš”)")

            await browser.close()
            return collected_count

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    import sys
    
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[êµë³´ì¦ê¶Œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            end_in = input("ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            sys.exit(0)

    scraper = KyoboScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
