import sys
import os

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (base ëª¨ë“ˆ import ìœ„í•¨)
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from base import AsyncBaseScraper
import logging
import re
from playwright.async_api import Page
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class KDIScraper(AsyncBaseScraper):
    """
    [KDI í•œêµ­ê°œë°œì—°êµ¬ì›] ë°ì´í„° ìŠ¤í¬ë˜í¼
    
    ìˆ˜ì§‘ ëŒ€ìƒ: ì •ì±…ìë£Œì‹¤ - êµ­í† ê°œë°œ ë¶„ì•¼
    ìˆ˜ì§‘ í•­ëª©: ì œëª©, ë‚ ì§œ, PDF ë‹¤ìš´ë¡œë“œ URL
    ë¡œì§: UI ì¡°ì‘ ë°©ì‹ (ì£¼ì œë³„ í•„í„° ì‚¬ìš©)
    """

    def __init__(self, start_date: str, end_date: str):
        super().__init__(start_date, end_date, "í•œêµ­ê°œë°œì—°êµ¬ì›")
        self.base_url = "https://eiec.kdi.re.kr"
        self.main_url = "https://eiec.kdi.re.kr/policy/materialList.do?depth1=M0000&depth2=A&search_txt=&topic=&pg=1&pp=20&type=J&device=pc"
        
    async def scrape(self, page: Page) -> int:
        collected_count = 0
        max_pages = 5
        
        try:
            logger.info(f"[{self.site_name}] ë©”ì¸ í˜ì´ì§€ ì ‘ì†: {self.main_url}")
            await page.goto(self.main_url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(8000)  # ì¶©ë¶„í•œ ëŒ€ê¸° ì‹œê°„
            
            # 1. ì£¼ì œë³„ í•„í„°ì—ì„œ "êµ­í† ê°œë°œ" ì„ íƒ
            logger.info("ì£¼ì œë³„ í•„í„°ì—ì„œ 'êµ­í† ê°œë°œ' ì„ íƒ ì‹œë„...")
            try:
                # ì£¼ì œ ë“œë¡­ë‹¤ìš´ ì°¾ê¸°
                subject_select = await page.query_selector('select[name*="topic"], select[name*="subject"], #selectSubject, select#topic')
                
                if subject_select:
                    # "êµ­í† ê°œë°œ" ì˜µì…˜ ì„ íƒ
                    await subject_select.select_option(label="êµ­í† ê°œë°œ")
                    logger.info("âœ… 'êµ­í† ê°œë°œ' ì„ íƒ ì™„ë£Œ")
                    await page.wait_for_timeout(1000)
                    
                    # ê²€ìƒ‰/ì ìš© ë²„íŠ¼ í´ë¦­
                    search_btn = await page.query_selector('button:has-text("ê²€ìƒ‰"), button:has-text("ì ìš©"), input[type="submit"]')
                    if search_btn:
                        await search_btn.click()
                        logger.info("âœ… ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
                        await page.wait_for_timeout(3000)
                else:
                    logger.warning("âš ï¸ ì£¼ì œ í•„í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - ì „ì²´ ë°ì´í„°ì—ì„œ í•„í„°ë§í•©ë‹ˆë‹¤")
            except Exception as e:
                logger.warning(f"âš ï¸ ì£¼ì œ í•„í„° ì„ íƒ ì‹¤íŒ¨: {e} - ì „ì²´ ë°ì´í„°ì—ì„œ í•„í„°ë§í•©ë‹ˆë‹¤")
            
            # 2. í˜ì´ì§€ë„¤ì´ì…˜ ìˆœíšŒ
            current_page = 1
            
            while current_page <= max_pages:
                logger.info(f"[{self.site_name}] í˜ì´ì§€ {current_page}/{max_pages} ì²˜ë¦¬ ì¤‘...")
                
                # ê²Œì‹œë¬¼ ëª©ë¡ ì¶”ì¶œ
                items = await page.query_selector_all('li a[href*="materialView"], tr td a[href*="materialView"], .list-item a[href*="view"]')
                
                if not items:
                    logger.info("ë” ì´ìƒ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                logger.info(f"   {len(items)}ê°œ ê²Œì‹œë¬¼ ë°œê²¬")
                
                # ë°ì´í„° ìˆ˜ì§‘
                targets = []
                for item in items:
                    try:
                        title = (await item.text_content()).strip()
                        href = await item.get_attribute('href')
                        
                        if not title or len(title) < 3 or not href:
                            continue
                        
                        # ë‚ ì§œ ì¶”ì¶œ (ë¶€ëª¨ ìš”ì†Œì—ì„œ)
                        date_str = "N/A"
                        parent = await item.evaluate_handle('el => el.closest("tr, li, .list-item")')
                        if parent:
                            parent_text = await parent.text_content()
                            # YYYY.MM.DD or YYYY-MM-DD íŒ¨í„´ ì°¾ê¸°
                            date_match = re.search(r'(\d{4}[.\-/]\d{2}[.\-/]\d{2})', parent_text)
                            if date_match:
                                date_str = date_match.group(1).replace('.', '-').replace('/', '-')
                        
                        # ë‚ ì§œ í•„í„°ë§
                        if date_str != "N/A":
                            if not self.is_in_period(date_str):
                                continue
                        
                        # URL êµ¬ì„±
                        full_url = urljoin(self.base_url, href)
                        
                        targets.append({
                            'title': title,
                            'date': date_str,
                            'url': full_url
                        })
                    except Exception as e:
                        logger.debug(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                if not targets:
                    logger.info(f"í˜ì´ì§€ {current_page}: ê¸°ê°„ ë‚´ í•­ëª© ì—†ìŒ")
                    break
                
                # í…ŒìŠ¤íŠ¸ìš©: ì²˜ìŒ 3ê±´ë§Œ ì²˜ë¦¬
                targets = targets[:3]
                logger.info(f"í˜ì´ì§€ {current_page}: {len(targets)}ê±´ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
                
                # 3. ìƒì„¸ í˜ì´ì§€ ìˆœíšŒ ë° PDF URL ìˆ˜ì§‘
                for idx, t in enumerate(targets, 1):
                    try:
                        logger.info(f"   [{idx}/{len(targets)}] {t['title'][:40]}... ì²˜ë¦¬ ì¤‘")
                        
                        # ëœë¤ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€ (ë´‡ ê°ì§€ ìš°íšŒ)
                        import random
                        wait_time = random.randint(4000, 7000)  # 4~7ì´ˆ
                        await page.wait_for_timeout(wait_time)
                        
                        # ìƒì„¸ í˜ì´ì§€ ì´ë™
                        await page.goto(t['url'], wait_until='domcontentloaded', timeout=20000)
                        await page.wait_for_timeout(2000)
                        
                        # HTML ì „ì²´ ê°€ì ¸ì˜¤ê¸° (KIF ë°©ì‹)
                        content = await page.content()
                        
                        # "ì •ìƒì ì¸ ìš”ì²­ì´ ì•„ë‹™ë‹ˆë‹¤" ì˜¤ë¥˜ í™•ì¸
                        if "ì •ìƒì ì¸ ìš”ì²­ì´ ì•„ë‹™ë‹ˆë‹¤" in content:
                            logger.warning(f"   âš ï¸ ì ‘ê·¼ ì°¨ë‹¨ - 15~20ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„")
                            wait_retry = random.randint(15000, 20000)
                            await page.wait_for_timeout(wait_retry)
                            await page.goto(t['url'], wait_until='domcontentloaded', timeout=20000)
                            await page.wait_for_timeout(3000)
                            
                            content = await page.content()
                            if "ì •ìƒì ì¸ ìš”ì²­ì´ ì•„ë‹™ë‹ˆë‹¤" in content:
                                logger.error(f"   âŒ ì¬ì‹œë„ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                                continue
                        
                        pdf_url = "N/A"
                        
                        # 4. PDF URL ì¶”ì¶œ (KIF ë°©ì‹: HTML íŒŒì‹±)
                        # KDI íŒ¨í„´: callDownload(num, filenum) ë˜ëŠ” onclick="window.location.href='callDownload.do?...'"
                        
                        # íŒ¨í„´ 1: callDownload í•¨ìˆ˜ í˜¸ì¶œ
                        # ì˜ˆ: onclick="callDownload('276492', '1')" ë˜ëŠ” callDownload(276492, 1)
                        match = re.search(r"callDownload\(['\"]?(\d+)['\"]?,\s*['\"]?(\d+)['\"]?\)", content)
                        
                        if match:
                            num, filenum = match.groups()
                            # dtimeì€ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ìƒì„±
                            import datetime
                            dtime = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                            pdf_url = f"{self.base_url}/policy/callDownload.do?num={num}&filenum={filenum}&dtime={dtime}"
                            logger.info(f"   âœ… [HTML íŒŒì‹±] callDownload URL êµ¬ì„±: num={num}, filenum={filenum}")
                        
                        # íŒ¨í„´ 2: ì§ì ‘ ë§í¬ (fallback)
                        if pdf_url == "N/A":
                            match2 = re.search(r"callDownload\.do\?([^'\">\s]+)", content)
                            if match2:
                                params = match2.group(1)
                                pdf_url = f"{self.base_url}/policy/callDownload.do?{params}"
                                logger.info(f"   âœ… [HTML íŒŒì‹±] ì§ì ‘ ë§í¬ ë°œê²¬")
                        
                        # íŒ¨í„´ 3: href ì†ì„±ì—ì„œ ì§ì ‘ ì¶”ì¶œ
                        if pdf_url == "N/A":
                            links = await page.query_selector_all('a[href*="callDownload"]')
                            for link in links:
                                href = await link.get_attribute('href')
                                if href:
                                    pdf_url = urljoin(self.base_url, href)
                                    logger.info(f"   âœ… [HTML ë§í¬] callDownload ë°œê²¬")
                                    break
                        
                        if pdf_url == "N/A":
                            logger.warning(f"   âš ï¸ PDF URL ì¶”ì¶œ ì‹¤íŒ¨")
                        
                        # ê²°ê³¼ ì €ì¥
                        self.save_result(t['title'], t['date'], pdf_url, t['url'])
                        collected_count += 1
                        logger.info(f"   [ìˆ˜ì§‘] {t['date']} | {t['title'][:30]}... | PDF: {bool(pdf_url!='N/A')}")
                        
                        # ìš”ì²­ ê°„ê²© (ë´‡ ì°¨ë‹¨ ë°©ì§€)
                        await page.wait_for_timeout(2000)
                        
                    except Exception as e:
                        logger.error(f"   ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}")
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                current_page += 1
                
                if current_page <= max_pages:
                    # í˜ì´ì§€ë„¤ì´ì…˜ ë²„íŠ¼ í´ë¦­
                    try:
                        # ëª©ë¡ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸° (pg íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                        next_url = f"https://eiec.kdi.re.kr/policy/materialList.do?depth1=M0000&depth2=A&search_txt=&topic=&pg={current_page}&pp=20&type=J&device=pc"
                        await page.goto(next_url, wait_until='domcontentloaded', timeout=20000)
                        await page.wait_for_timeout(2000)
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                        break
                        
        except Exception as e:
            logger.error(f"ì „ì²´ ì˜¤ë¥˜: {e}")
            
        # ---------------------------------------------------------
        # 2ì°¨ ìˆ˜ì§‘: KDI í† í”½ (www.kdi.re.kr/research/topicList?cd=A)
        # ---------------------------------------------------------
        try:
            topic_url = "https://www.kdi.re.kr/research/topicList?cd=A"
            logger.info(f"[{self.site_name}] 2ì°¨ ìˆ˜ì§‘ ì‹œì‘: KDI í† í”½ ({topic_url})")
            
            await page.goto(topic_url, wait_until='domcontentloaded', timeout=20000)
            await page.wait_for_timeout(3000)
            
            # ëª©ë¡ ì•„ì´í…œ ì¶”ì¶œ (.list_type_new > li ë“±)
            topic_items = await page.query_selector_all('.list_type_new > li, .board_list > li')
            logger.info(f"   [KDI í† í”½] ëª©ë¡ {len(topic_items)}ê°œ ë°œê²¬")
            
            for item in topic_items[:10]: # ìƒìœ„ 10ê°œë§Œ ì‹œë„
                try:
                    # ì œëª© ë° ë§í¬
                    a_tag = await item.query_selector('a.tit, .txt_box > a, dt > a')
                    if not a_tag: continue
                    
                    t_title = await a_tag.text_content()
                    t_href = await a_tag.get_attribute('href')
                    t_full_url = urljoin("https://www.kdi.re.kr", t_href)
                    
                    # ë‚ ì§œ í™•ì¸
                    d_tag = await item.query_selector('.date, span.date, .dt')
                    if d_tag:
                        t_date = (await d_tag.text_content()).strip().replace('.', '-')
                        # ê¸°ê°„ ì²´í¬
                        if not self.is_in_period(t_date):
                            continue
                    else:
                        t_date = "N/A"
                    
                    # ìƒì„¸ í˜ì´ì§€ ì´ë™
                    await page.goto(t_full_url, wait_until='domcontentloaded', timeout=15000)
                    await page.wait_for_timeout(1500)
                    
                    # [ì‚¬ìš©ì ìš”ì²­] ì›ë¬¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ íŒŒì‹±
                    # <button type="button" class="i02" onclick="location.href='/file/download...'">
                    
                    t_pdf_url = "N/A"
                    dw_btn = await page.query_selector('button[onclick*="/file/download"], a[href*="/file/download"]')
                    
                    if dw_btn:
                        # 1. onclick ì†ì„±ì—ì„œ ì¶”ì¶œ
                        onclick_val = await dw_btn.get_attribute('onclick')
                        if onclick_val:
                            # location.href='...' íŒ¨í„´ ì¶”ì¶œ
                            m = re.search(r"location\.href=['\"]([^'\"]+)['\"]", onclick_val)
                            if m:
                                t_pdf_url = urljoin("https://www.kdi.re.kr", m.group(1))
                        
                        # 2. href ì†ì„±ì—ì„œ ì¶”ì¶œ (a íƒœê·¸ì¼ ê²½ìš°)
                        if t_pdf_url == "N/A":
                            href_val = await dw_btn.get_attribute('href')
                            if href_val and "/file/download" in href_val:
                                t_pdf_url = urljoin("https://www.kdi.re.kr", href_val)
                    
                    if t_pdf_url != "N/A":
                         logger.info(f"   âœ… [KDI í† í”½] PDF URL ì¶”ì¶œ ì„±ê³µ: {t_pdf_url}")
                    
                    self.save_result(t_title.strip(), t_date, t_pdf_url, t_full_url)
                    collected_count += 1
                    
                    # ëª©ë¡ìœ¼ë¡œ ë³µê·€ (history backì´ ë¹ ë¦„)
                    await page.go_back()
                    await page.wait_for_timeout(1000)
                    
                except Exception as e:
                    logger.warning(f"   [KDI í† í”½] ì•„ì´í…œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    # ë³µêµ¬ë¥¼ ìœ„í•´ ë‹¤ì‹œ ëª©ë¡ìœ¼ë¡œ ì´ë™ ì‹œë„
                    if page.url != topic_url:
                        await page.goto(topic_url, wait_until='domcontentloaded')
        
        except Exception as e:
            logger.error(f"[KDI í† í”½] 2ì°¨ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return collected_count

    async def scrape_all(self):
        """í†µí•© ìˆ˜ì§‘ê¸° í˜¸ì¶œìš© ì§„ì…ì """
        from playwright.async_api import async_playwright
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            # BaseScraperì˜ _setup_page í™œìš© ê°€ëŠ¥í•˜ë©´ ì¢‹ê² ì§€ë§Œ, 
            # ì—¬ê¸°ì„œëŠ” ì§ì ‘ ìƒì„±í•˜ê±°ë‚˜ _setup_page í˜¸ì¶œ
            page = await self._setup_page(context)
            
            await self.scrape(page)
            
            await browser.close()

if __name__ == "__main__":
    import sys
    import asyncio
    
    # ìœˆë„ìš° ì¸ì½”ë”© ì„¤ì •
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except:
            pass
    
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[KDI ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_date = input("ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD): ").strip()
            if not start_date:
                start_date = "2024-01-01" # Default
                
            end_date = input("ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD): ").strip()
            if not end_date:
                import datetime
                end_date = datetime.datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            print("\nì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)
    
    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    
    scraper = KDIScraper(start_date, end_date)
    asyncio.run(scraper.scrape_all())
