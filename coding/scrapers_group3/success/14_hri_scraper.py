#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í˜„ëŒ€ê²½ì œì—°êµ¬ì› (HRI) ìŠ¤í¬ë˜í¼
- ëŒ€ìƒ: ì—°êµ¬ë³´ê³ ì„œ (ê²½ì œ, ì‚°ì—…ê²½ì˜, í†µì¼ê²½ì œ)
- URL: https://www.hri.co.kr/kor/report/report.html?mode=1 (2, 3)
"""

import sys
import os
import asyncio
import logging
import json
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin
from playwright.async_api import async_playwright

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("HRI")

class HRIScraper:
    def __init__(self, start_date=None, end_date=None):
        self.base_url = "https://www.hri.co.kr"
        # mode=1: ê²½ì œ, mode=2: ì‚°ì—…ê²½ì˜, mode=3: í†µì¼ê²½ì œ
        self.modes = [
            (1, "ê²½ì œ"),
            (2, "ì‚°ì—…ê²½ì˜"),
            (3, "í†µì¼ê²½ì œ")
        ]
        
        self.start_date = self._parse_date(start_date)
        self.end_date = self._parse_date(end_date)
        
        self.results = []
        self.output_dir = os.path.join(os.path.dirname(__file__), "output")
        if not os.path.exists(self.output_dir):
             self.output_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "output"))
        os.makedirs(self.output_dir, exist_ok=True)

    def _parse_date(self, date_str):
        if not date_str: return None
        try: return datetime.strptime(date_str, "%Y-%m-%d")
        except: return None

    def _is_in_period(self, date_obj):
        if not date_obj: return False
        if self.start_date and date_obj < self.start_date: return False
        if self.end_date and date_obj > self.end_date: return False
        return True

    async def scrape(self):
        logger.info("ğŸ“¦ í˜„ëŒ€ê²½ì œì—°êµ¬ì› (HRI) ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {self.start_date.strftime('%Y-%m-%d') if self.start_date else 'N/A'} ~ {self.end_date.strftime('%Y-%m-%d') if self.end_date else 'N/A'}")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            for mode, mode_name in self.modes:
                await self.scrape_category(context, mode, mode_name)
                
            await browser.close()
        
        self.save_files()
        logger.info("âœ… HRI ìˆ˜ì§‘ ì™„ë£Œ!")

    async def scrape_category(self, context, mode, mode_name):
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š [{mode_name}] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹œì‘ (mode={mode})")
        
        page = await context.new_page()
        page_num = 1
        
        while True:
            url = f"{self.base_url}/kor/report/report.html?mode={mode}&page={page_num}"
            logger.info(f"   í˜ì´ì§€ ì´ë™: {url}")
            
            try:
                await page.goto(url, wait_until="networkidle", timeout=30000)
            except Exception as e:
                logger.error(f"   í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                break
            
            items = await page.query_selector_all('a.item')
            if not items:
                logger.info("   ë” ì´ìƒ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                break
                
            logger.info(f"   ë°œê²¬ëœ í•­ëª©: {len(items)}ê°œ")
            
            should_stop = False
            page_collected = 0
            
            # í˜„ì¬ í˜ì´ì§€ì˜ í•­ëª©ë“¤ì„ ìˆœíšŒ
            # ì£¼ì˜: ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ë©´ ëª©ë¡ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ê°€ ë°”ë€Œë¯€ë¡œ,
            # URLê³¼ ë©”íƒ€ë°ì´í„°ë§Œ ë¨¼ì € ì¶”ì¶œí•˜ê³  ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸ì€ ë³„ë„ë¡œ í•˜ê±°ë‚˜ ìƒˆ íƒ­ì„ ì‚¬ìš©í•´ì•¼ í•¨.
            # ì—¬ê¸°ì„œëŠ” ëª©ë¡ì—ì„œ ê¸°ë³¸ ì •ë³´ë¥¼ ë¨¼ì € ì¶”ì¶œ.
            
            current_page_items = []
            
            for item in items:
                try:
                    # ìƒì„¸ URL
                    href = await item.get_attribute('href')
                    if not href: continue
                    detail_url = urljoin(url, href)
                    
                    # ì œëª©
                    title_elem = await item.query_selector('.tit .tit-text')
                    title = (await title_elem.text_content()).strip() if title_elem else ""
                    
                    # ë‚ ì§œ (ë°œê°„ì¼ ì°¾ê¸°)
                    date_val = None
                    info_lists = await item.query_selector_all('.info .list')
                    for info_item in info_lists:
                        tit_el = await info_item.query_selector('.info-tit')
                        if tit_el and "ë°œê°„ì¼" in (await tit_el.text_content()):
                            val_el = await info_item.query_selector('.info-text')
                            if val_el:
                                date_text = (await val_el.text_content()).strip()
                                # ë‚ ì§œ íŒŒì‹± (2026-01-02)
                                try:
                                    date_val = datetime.strptime(date_text, "%Y-%m-%d")
                                except:
                                    pass
                            break
                    
                    if not date_val: continue
                    
                    # ë‚ ì§œ ì²´í¬
                    if self.end_date and date_val > self.end_date:
                        continue # ì•„ì§ ê¸°ê°„ ì „ (ë¯¸ë˜)
                    
                    if self.start_date and date_val < self.start_date:
                        should_stop = True # ê³¼ê±° ë°ì´í„° ë„ë‹¬
                        break
                        
                    current_page_items.append({
                        'title': title,
                        'date': date_val,
                        'detail_url': detail_url
                    })
                    
                except Exception as e:
                    logger.error(f"   í•­ëª© íŒŒì‹± ì—ëŸ¬: {e}")
                    continue
            
            # ì¶”ì¶œëœ í•­ëª©ë“¤ì— ëŒ€í•´ ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ PDF ë§í¬ ìˆ˜ì§‘
            for item_data in current_page_items:
                pdf_url = await self.get_params_from_detail(context, item_data['detail_url'])
                if pdf_url:
                    date_str = item_data['date'].strftime("%Y-%m-%d")
                    logger.info(f"   âœ… {item_data['title'][:40]}... ({date_str})")
                    self.results.append({
                        'source': f'HRI_{mode_name}',
                        'title': item_data['title'],
                        'date': date_str,
                        'download_url': pdf_url,
                        'collected_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
                    page_collected += 1
                
            if should_stop:
                logger.info("   ì„¤ì •ëœ ê¸°ê°„ ì´ì „ì˜ ë°ì´í„°ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            if page_collected == 0 and len(current_page_items) == 0:
                 # ì´ë²ˆ í˜ì´ì§€ì—ì„œ ìœ íš¨í•œ ë‚ ì§œê°€ ì—†ìœ¼ë©´ì„œ should_stopë„ ì•ˆ ê±¸ë ¸ë‹¤ë©´? (ì „ë¶€ ë¯¸ë˜ ë‚ ì§œì¸ ê²½ìš° ë“±)
                 # í•˜ì§€ë§Œ ëª©ë¡ì€ ë³´í†µ ìµœì‹ ìˆœì´ë¯€ë¡œ, ì „ë¶€ ë¯¸ë˜ë©´ ê³„ì† ì§„í–‰í•´ì•¼ í•  ìˆ˜ë„ ìˆê³ , ì „ë¶€ ê³¼ê±°ë©´ should_stop.
                 # HRI ë¦¬ìŠ¤íŠ¸ëŠ” 'ë°œê°„ì¼ìˆœ' ì •ë ¬ì´ ê¸°ë³¸ì´ë¯€ë¡œ,
                 # ì²« í•­ëª©ì´ start_dateë³´ë‹¤ ì‘ìœ¼ë©´(ê³¼ê±°ë©´) ì¢…ë£Œê°€ ë§ìŒ.
                 # ë§ˆì§€ë§‰ í•­ëª©ì´ end_dateë³´ë‹¤ í¬ë©´(ë¯¸ë˜ë©´) ë‹¤ìŒ í˜ì´ì§€ë„ ë´ì•¼ í•¨.
                 pass

            page_num += 1
            await asyncio.sleep(1) # ë¶€í•˜ ì¡°ì ˆ
            
        await page.close()

    async def get_params_from_detail(self, context, detail_url):
        """ìƒì„¸ í˜ì´ì§€ -> ë‹¤ìš´ë¡œë“œ íŒì—… -> ì‹¤ì œ PDF ë§í¬ ì¶”ì¶œ"""
        page = await context.new_page()
        try:
            # 1. ìƒì„¸ í˜ì´ì§€ ì´ë™
            await page.goto(detail_url, wait_until="networkidle", timeout=20000)
            
            # 2. ë‹¤ìš´ë¡œë“œ íŒì—… ë§í¬ ì°¾ê¸°
            download_btn = await page.query_selector('a.popup-link.link[href*="file-download.html"]')
            if not download_btn:
                return None
                
            popup_href = await download_btn.get_attribute('href')
            popup_url = urljoin(detail_url, popup_href)
            
            # 3. íŒì—… í˜ì´ì§€ë¡œ ì´ë™
            await page.goto(popup_url, wait_until="networkidle", timeout=20000)
            
            # 4. ì‹¤ì œ PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            # ì‚¬ìš©ì ì œê³µ: <a class="d-block link my-2" ...>
            real_download_link = await page.query_selector('a.d-block.link, a[href*=".pdf"], a[download]')
            
            if real_download_link:
                file_href = await real_download_link.get_attribute('href')
                if file_href:
                    # ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° ì²˜ë¦¬ (/upload/...)
                    full_pdf_url = urljoin(self.base_url, file_href)
                    logger.info(f"   âœ… ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {full_pdf_url}")
                    return full_pdf_url
            
            logger.warning(f"   âš ï¸ íŒì—… ë‚´ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨: {popup_url}")
            return None
            
        except Exception as e:
            logger.error(f"   ìƒì„¸ í˜ì´ì§€ ì—ëŸ¬ ({detail_url}): {e}")
            return None
        finally:
            await page.close()

    def save_files(self):
        if not self.results:
            logger.warning("âš ï¸ ë°ì´í„° ì—†ìŒ")
            return
            
        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_results = []
        for item in self.results:
            url = item['download_url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(item)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = os.path.join(self.output_dir, f"hri_results_{timestamp}.json")
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(unique_results, f, ensure_ascii=False, indent=4)
        
        logger.info(f"ğŸ’¾ ì €ì¥: {json_path} ({len(unique_results)}ê±´)")

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n" + "="*50)
        print("í˜„ëŒ€ê²½ì œì—°êµ¬ì› (HRI) ìŠ¤í¬ë˜í¼")
        print("="*50)
        try:
            today = datetime.now().strftime("%Y-%m-%d")
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            
            end_in = input(f"ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else today
        except KeyboardInterrupt:
            sys.exit(0)
            
    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    scraper = HRIScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
