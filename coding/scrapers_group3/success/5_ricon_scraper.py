#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëŒ€í•œê±´ì„¤ì •ì±…ì—°êµ¬ì›(RICON) ìŠ¤í¬ë˜í¼
- URL: https://www.ricon.re.kr/board/list.php?group=issue&page=economic_index&cate=9
- ëŒ€ìƒ: ê±´ì„¤ê²½ì œì§€í‘œ
- ë‚ ì§œ í•„í„°ë§ ì§€ì›: YYYY-MM-DD
"""

import sys
import os
import asyncio
import logging
import csv
import json
import re
from datetime import datetime
from urllib.parse import urljoin

from playwright.async_api import async_playwright, Page

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(os.path.join(current_dir, 'ricon_scraper.log'), encoding='utf-8'),
        logging.StreamHandler()  
    ]
)
logger = logging.getLogger("RICON")

class RICONScraper:
    def __init__(self, start_date=None, end_date=None):
        self.base_url = "https://www.ricon.re.kr"
        self.target_url = "https://www.ricon.re.kr/board/list.php?group=issue&page=economic_index&cate=9"
        
        self.start_date = self._parse_date(start_date)
        self.end_date = self._parse_date(end_date)
        
        self.results = []
        self.output_dir = os.path.join(project_root, "output")
        os.makedirs(self.output_dir, exist_ok=True)

    def _parse_date(self, date_str):
        if not date_str:
            return None
        try:
            return datetime.strptime(date_str, "%Y-%m-%d")
        except ValueError:
            logger.error(f"âŒ ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {date_str} (YYYY-MM-DD í˜•ì‹ì´ì–´ì•¼ í•¨)")
            return None

    def _is_in_period(self, date_str):
        if not date_str or date_str == "N/A":
            return False
            
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
            date_str = date_str.strip().replace('.', '-').replace('/', '-')
            if "ë…„" in date_str: # 2026ë…„ 1ì›” 1ì¼ ì²˜ë¦¬
                result = re.search(r'(\d{4})[^0-9]+(\d{1,2})[^0-9]+(\d{1,2})', date_str)
                if result:
                    date_str = f"{result.group(1)}-{result.group(2).zfill(2)}-{result.group(3).zfill(2)}"
            
            # YYYY-MM-DD ì¶”ì¶œ
            match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
            if match:
                current_date = datetime.strptime(match.group(0), "%Y-%m-%d")
            else:
                return False
                
            if self.start_date and current_date < self.start_date:
                return False
            if self.end_date and current_date > self.end_date:
                return False
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {e}")
            return False

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(
                viewport={"width": 1280, "height": 720},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            logger.info(f"ğŸŒ ì ‘ì† ì¤‘: {self.target_url}")
            await page.goto(self.target_url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(2000)

            total_collected = 0
            max_pages = 50 # ì¶©ë¶„íˆ ì„¤ì •
            current_page = 1
            
            while current_page <= max_pages:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {current_page} ë¶„ì„ ì¤‘...")
                
                # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
                rows = await page.query_selector_all('table tbody tr') # RICONì€ table êµ¬ì¡°
                if not rows:
                     rows = await page.query_selector_all('.board-list > li') # ë°±ì—…
                
                if not rows:
                    logger.warning("âŒ ê²Œì‹œë¬¼ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                logger.info(f"   ğŸ” ê²Œì‹œë¬¼ {len(rows)}ê°œ ë°œê²¬")
                page_collected_count = 0
                
                for row in rows:
                    try:
                        # ë‚ ì§œ ì¶”ì¶œ
                        date_text = "N/A"
                        date_elem = await row.query_selector('td:nth-child(3), .date, td.date')
                        if date_elem:
                            date_text = await date_elem.text_content()
                            
                        # ë‚ ì§œê°€ ë‚´ìš©ì— ìˆ¨ì–´ìˆì„ ê²½ìš° (ëª¨ë°”ì¼ ë·° ë“±)
                        if "20" not in date_text:
                            text_all = await row.text_content()
                            match = re.search(r'20\d{2}[.-]\d{2}[.-]\d{2}', text_all)
                            if match:
                                date_text = match.group(0)

                        if not self._is_in_period(date_text):
                            continue
                            
                        # ì œëª© ì¶”ì¶œ
                        title_elem = await row.query_selector('a')
                        if not title_elem:
                            continue
                        
                        title_text = (await title_elem.text_content()).strip()
                        href = await title_elem.get_attribute('href')
                        
                        if not href or "javascript" in href:
                            continue
                            
                        detail_url = urljoin(self.base_url, href)
                        
                        # --- ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ---
                        pdf_url = await self._scrape_detail(context, detail_url)
                        
                    self.results.append({
                        'source': 'RICON',
                        'title': title_text,
                        'date': date_text,
                        # 'link': detail_url, (ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì œê±°)
                        'pdf_url': pdf_url,
                        'collected_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    })
                    
                    # ë”ë¸” https ì²´í¬ ë° ìˆ˜ì •
                    if pdf_url.startswith('https://https://'):
                        pdf_url = pdf_url.replace('https://https://', 'https://')
                    
                    logger.info(f"   âœ… [ìˆ˜ì§‘] {date_text} | {title_text[:15]}... | PDF: {'O' if pdf_url != 'N/A' else 'X'}")
                    total_collected += 1
                    page_collected_count += 1
                    
                except Exception as e:
                    logger.error(f"   âš ï¸ í•­ëª© ì—ëŸ¬: {e}")
                    continue
                
                # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
                if page_collected_count == 0 and len(self.results) > 0:
                     # ì´ë²ˆ í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘ëœ ê²Œ ì—†ê³  ì´ë¯¸ ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¢…ë£Œ (ë‚ ì§œ ë²”ìœ„ ë²—ì–´ë‚¨)
                     logger.info("   â¹ï¸ ë‚ ì§œ ë²”ìœ„ ì´ˆê³¼ë¡œ ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ")
                     break

                # í˜ì´ì§€ë„¤ì´ì…˜ (ë‹¤ìŒ ë²„íŠ¼ ì°¾ê¸°)
                # Next button: btn_next or page=N
                try:
                    next_btn = await page.query_selector('a.btn_next, a.next')
                    if not next_btn:
                        # ìˆ«ì ë²„íŠ¼ìœ¼ë¡œ ì´ë™ (í˜„ì¬+1 í˜ì´ì§€)
                        next_btn = await page.query_selector(f'a[href*="page={current_page + 1}"]')
                    
                    if next_btn:
                        await next_btn.click(force=True) # force=Trueë¡œ ê°€ë ¤ì§ ë°©ì§€
                        await page.wait_for_timeout(3000) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                        current_page += 1
                    else:
                        logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                        break
                except Exception as e:
                    logger.warning(f"   âš ï¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ë§ˆì§€ë§‰ì¼ ìˆ˜ ìˆìŒ): {e}")
                    break

            await browser.close()
            
            logger.info(f"\nğŸ‰ ì´ {total_collected}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            self.save_files()

    async def _scrape_detail(self, context, url):
        """ìƒˆ íƒ­ì—ì„œ ìƒì„¸ í˜ì´ì§€ ì—´ê³  PDF ë§í¬ ì¶”ì¶œ (ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì‹œë„)"""
        page = await context.new_page()
        pdf_url = "N/A"
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            
            # 1. file_download.php ë§í¬ ì°¾ê¸°
            download_link = await page.query_selector('a[href*="file_download.php"]')
            
            # 2. ì—†ë‹¤ë©´ ì¼ë°˜ PDF/ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            if not download_link:
                candidates = await page.query_selector_all('a[href]')
                for lnk in candidates:
                    txt = await lnk.text_content()
                    hr = await lnk.get_attribute('href')
                    if hr and not hr.startswith('javascript') and ('pdf' in txt.lower() or 'download' in hr.lower()):
                        download_link = lnk
                        break
            
            if download_link:
                # 1. ìƒˆ ì°½(Popup) ê°ì§€ ì‹œë„ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: ìƒˆë¡œ ëœ¨ëŠ” ì°½ì˜ URL)
                try:
                    async with page.expect_popup(timeout=5000) as popup_info:
                        await download_link.click()
                    
                    popup = await popup_info.value
                    # ë¦¬ë””ë ‰ì…˜ ê°€ëŠ¥ì„± ê³ ë ¤í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ê°€ ì ì í•´ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
                    await popup.wait_for_load_state("networkidle")
                    
                    pdf_url = popup.url
                    # ì‚¬ìš©ìê°€ ì›í•˜ëŠ” 'ìƒˆ ì°½ URL'ì„ì„ ëª…ì‹œ
                    logger.info(f"   [ì„±ê³µ] ìƒˆ ì°½(Popup) ìµœì¢… URL í¬ì°©: {pdf_url}")
                    await popup.close()
                    
                except Exception:
                    # 2. íŒì—… ì•„ë‹ˆë©´ ë‹¤ìš´ë¡œë“œ ì´ë²¤íŠ¸ ì‹œë„
                    try:
                         # ì´ë¯¸ í´ë¦­ì„ í–ˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë‹¤ì‹œ í´ë¦­ ì‹œë„í•˜ì§€ ì•Šê³  ì´ë²¤íŠ¸ë§Œ ê¸°ë‹¤ë ¤ë³´ê±°ë‚˜
                         # í´ë¦­ì´ ì”¹í˜”ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë‹¤ì‹œ ì‹œë„
                         async with page.expect_download(timeout=3000) as download_info:
                             await download_link.click()
                         
                         download = await download_info.value
                         pdf_url = download.url
                         await download.cancel()
                         logger.info(f"   [ë‹¤ìš´ë¡œë“œ ê°ì§€] URL: {pdf_url}")
                         
                    except Exception:
                         # 3. ëª¨ë‘ ì‹¤íŒ¨ ì‹œ href ë°±ì—…
                         raw = await download_link.get_attribute('href')
                         if raw:
                             pdf_url = urljoin(self.base_url, raw)
                             logger.info(f"   [ë§í¬ ì¶”ì¶œ(ë°±ì—…)] {pdf_url}")
                        
        except Exception as e:
            logger.debug(f"ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        finally:
            await page.close()
        return pdf_url

    def save_files(self):
        if not self.results:
            logger.warning("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # CSV ì €ì¥
        csv_filename = f"ricon_results_{timestamp}.csv"
        csv_path = os.path.join(self.output_dir, csv_filename)
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=self.results[0].keys())
            writer.writeheader()
            writer.writerows(self.results)
        logger.info(f"ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

        # JSON ì €ì¥
        json_filename = f"ricon_results_{timestamp}.json"
        json_path = os.path.join(self.output_dir, json_filename)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)
        logger.info(f"ğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: {json_path}")

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n" + "="*50)
        print("ëŒ€í•œê±´ì„¤ì •ì±…ì—°êµ¬ì› (RICON) ìŠ¤í¬ë˜í¼")
        print("="*50)
        try:
            today = datetime.now().strftime("%Y-%m-%d")
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            
            end_in = input(f"ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: {today}]: ").strip()
            end_date = end_in if end_in else today
        except KeyboardInterrupt:
            sys.exit(0)

    print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    scraper = RICONScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
