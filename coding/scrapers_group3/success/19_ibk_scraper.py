
import sys
import os
import asyncio
import re
from datetime import datetime
from urllib.parse import urljoin
from playwright.async_api import async_playwright

# ìƒìœ„ í´ë” ê²½ë¡œ ì„¤ì •
cur_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(cur_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

from base import AsyncBaseScraper

class IBKScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        super().__init__(start_date, end_date, "IBKíˆ¬ìì¦ê¶Œ")
        # ì£¼ì˜: HTTPSê°€ ì•„ë‹Œ HTTPë¡œ ì ‘ì†í•´ì•¼ í•¨
        self.base_url = "http://research.ibk.co.kr"
        self.pages = {
            "ê²½ì œë¶„ì„": "/research/board/economy-news/list",
            "íˆ¬ìì „ëµ": "/research/board/invest-strategy/list",
            "ì‚°ì—…ë¶„ì„": "/research/board/industry/list", 
            "ê¸°ì—…ë¶„ì„": "/research/board/company/list" 
        }

    async def _scrape_category(self, page, cat_name, url_path):
        full_url = urljoin(self.base_url, url_path)
        logger_prefix = f"   [{cat_name}]"
        print(f"{logger_prefix} ì´ë™: {full_url}")
        
        try:
            # í˜ì´ì§€ ì´ë™
            await page.goto(full_url, wait_until='networkidle', timeout=30000)
        except Exception as e:
            print(f"{logger_prefix} âš ï¸ ì ‘ì† ì‹¤íŒ¨: {e}")
            return 0

        collected_count = 0
        current_page = 1
        
        while True:
            print(f"{logger_prefix} í˜ì´ì§€ {current_page} ë¶„ì„ ì¤‘...")
            
            # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ëŒ€ê¸°
            try:
                # .subject í´ë˜ìŠ¤ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                await page.wait_for_selector('.subject', timeout=5000)
            except:
                print(f"{logger_prefix} âš ï¸ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë˜ëŠ” ë¡œë”© ì§€ì—°)")
                # 'ê²Œì‹œë¬¼ ì—†ìŒ' í…ìŠ¤íŠ¸ í™•ì¸ (í˜ì´ì§€ ì†ŒìŠ¤ ì „ì²´ì—ì„œ)
                content = await page.content()
                if "ë“±ë¡ëœ ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤" in content or "no data" in content:
                    print(f"{logger_prefix} ğŸ ê²Œì‹œë¬¼ ì—†ìŒ. ì¢…ë£Œ.")
                    break

            # ì•„ì´í…œ ì¶”ì¶œ
            # ë¶„ì„ ê²°ê³¼ .subjectê°€ 10ê°œ ë°œê²¬ë¨. ì´ë¥¼ í¬í•¨í•˜ëŠ” li ë˜ëŠ” trì„ ì°¾ìŒ.
            # li ì•ˆì— subjectê°€ ìˆëŠ” êµ¬ì¡°ê°€ ê°€ì¥ ìœ ë ¥í•¨.
            items = await page.query_selector_all('li:has(.subject)')
            
            # ë§Œì•½ li êµ¬ì¡°ê°€ ì•„ë‹ˆë¼ë©´ ê·¸ëƒ¥ .subjectë¥¼ ê°€ì§„ div ë“±ì„ ì°¾ìŒ (ì˜ˆë¹„)
            if not items:
                items = await page.query_selector_all('tr:has(.subject)')
            if not items:
                # ìµœí›„ì˜ ìˆ˜ë‹¨: .subject ìì²´ë¥¼ ì•„ì´í…œìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë¶€ëª¨/í˜•ì œ íƒìƒ‰
                items = await page.query_selector_all('.subject')
                if items:
                     print(f"{logger_prefix} â„¹ï¸ .subject ìš”ì†Œë¥¼ ì§ì ‘ ìˆœíšŒí•©ë‹ˆë‹¤.")

            if not items:
                print(f"{logger_prefix} âš ï¸ ì•„ì´í…œ 0ê°œ. ì¢…ë£Œ.")
                break
                
            print(f"{logger_prefix} ì•„ì´í…œ {len(items)}ê°œ ë°œê²¬")
            
            count_in_page = 0
            for item in items:
                try:
                    # ì œëª© ì¶”ì¶œ. item ìì²´ê°€ .subjectì¼ ìˆ˜ë„ ìˆê³  ì»¨í…Œì´ë„ˆì¼ ìˆ˜ë„ ìˆìŒ
                    title_elem = await item.query_selector('a')
                    # ë§Œì•½ itemì´ ì»¨í…Œì´ë„ˆë¼ë©´ .subject a ë¥¼ ì°¾ì•„ì•¼ í•¨
                    if await item.query_selector('.subject a'):
                        title_elem = await item.query_selector('.subject a')
                    
                    if not title_elem:
                        # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°? 
                        continue
                    
                    title = (await title_elem.text_content()).strip()
                    view_href = await title_elem.get_attribute('href')
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    # ê°™ì€ ì»¨í…Œì´ë„ˆ ë‚´ì˜ .date ë˜ëŠ” .meta
                    # itemì´ .subjectë¼ë©´ ë¶€ëª¨ë¡œ ì˜¬ë¼ê°€ì„œ ì°¾ì•„ì•¼ í•  ìˆ˜ë„ ìˆìŒ
                    date_text = "0000-00-00"
                    
                    # 1. ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²€ìƒ‰
                    date_elem = await item.query_selector('.date, .meta, .regDate')
                    if not date_elem:
                         # 2. í˜•ì œ ìš”ì†Œ ê²€ìƒ‰ (itemì´ .subjectì¸ ê²½ìš°) -> Playwrightì—ì„œëŠ” elementhandleì—ì„œ xpath .. ë¶ˆê°€.
                         # ë”°ë¼ì„œ ìœ„ì—ì„œ itemì„ ì¡ì„ ë•Œ ì»¨í…Œì´ë„ˆ(li)ë¥¼ ì¡ëŠ”ê²Œ ì¤‘ìš”í–ˆìŒ.
                         # ë§Œì•½ itemì´ lië¼ë©´ í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ì°¾ê¸°
                         full_txt = await item.text_content()
                         m = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', full_txt)
                         if m:
                             date_text = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
                    
                    if date_elem and date_text == "0000-00-00":
                        txt = (await date_elem.text_content()).strip()
                        m = re.search(r'(\d{4})[\.-](\d{2})[\.-](\d{2})', txt)
                        if m:
                            date_text = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"

                    # ë‚ ì§œ í•„í„°ë§
                    if not self.is_in_period(date_text):
                        if date_text != "0000-00-00" and date_text < str(self.start_date):
                            pass 
                        continue

                    # PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ
                    pdf_url = "N/A"
                    down_btn = await item.query_selector('a.file, a.btn-down, a[href*="download"], img[src*="pdf"]')
                    if down_btn:
                        # a íƒœê·¸ì¸ì§€ img íƒœê·¸ì¸ì§€ í™•ì¸
                        tag_name = await down_btn.evaluate("el => el.tagName")
                        if tag_name == "IMG":
                             # ì´ë¯¸ì§€ë¥¼ ê°ì‹¸ëŠ” a íƒœê·¸ ì°¾ê¸°
                             parent_a = await down_btn.evaluate_handle("el => el.closest('a')")
                             if parent_a:
                                 href = await parent_a.get_attribute('href')
                                 if href: pdf_url = urljoin(self.base_url, href)
                        else:
                            href = await down_btn.get_attribute('href')
                            if href:
                                pdf_url = urljoin(self.base_url, href)

                    full_view_url = urljoin(self.base_url, view_href) if view_href else page.url
                    
                    if pdf_url != "N/A":
                        print(f"      âœ… ìˆ˜ì§‘: {title[:15]}... ({date_text})")
                        self.save_result(title, date_text, pdf_url, full_view_url)
                        collected_count += 1
                        count_in_page += 1
                    
                except Exception as e:
                    print(f"      âš ï¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                    continue
            
            # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
            # .paging > a 
            next_page = current_page + 1
            
            # ìˆ«ì ë²„íŠ¼ í´ë¦­ ì‹œë„ (í…ìŠ¤íŠ¸ë¡œ ë§¤ì¹­)
            # ì •í™•íˆ ìˆ«ìë§Œ ìˆëŠ” ë§í¬ ì°¾ê¸°
            next_btn = await page.query_selector(f'.paging a:text-is("{next_page}")')
            
            if not next_btn:
                # ë‹¤ìŒ í™”ì‚´í‘œ ë²„íŠ¼ (ë³´í†µ alt="ë‹¤ìŒ" ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ê±°ë‚˜ classê°€ next)
                next_btn = await page.query_selector('.paging a.next, .paging .btn_next')
                
            if not next_btn:
                print(f"{logger_prefix} ğŸ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì—†ìŒ ({next_page}). ì¢…ë£Œ.")
                break
            
            try:
                await next_btn.click()
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(1000)
                current_page += 1
            except Exception as e:
                print(f"{logger_prefix} âš ï¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
                break

        return collected_count

    async def scrape(self):
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ëŸ°ì¹­
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 720},
                ignore_https_errors=True
            )
            page = await context.new_page()
            
            print(f"ğŸš€ [IBKíˆ¬ìì¦ê¶Œ] ìˆ˜ì§‘ ì‹œì‘ ({self.start_date} ~ {self.end_date})")
            
            total_count = 0
            for cat_name, path in self.pages.items():
                print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ ì‹œì‘: {cat_name}")
                count = await self._scrape_category(page, cat_name, path)
                total_count += count
                
            print(f"\nğŸ [IBKíˆ¬ìì¦ê¶Œ] ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {total_count}ê±´")
            await browser.close()

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass
        
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[IBKíˆ¬ìì¦ê¶Œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            start_date = start_in if start_in else "2024-01-01"
            end_in = input("ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            sys.exit(0)
        
    scraper = IBKScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
    
    if scraper.results:
        import json
        output_dir = os.path.join(cur_dir, "output")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"ibk_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(output_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(scraper.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
