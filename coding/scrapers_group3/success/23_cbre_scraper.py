import asyncio
import logging
import re
from datetime import datetime
from urllib.parse import urljoin
import os
import sys

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ base.pyë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•¨ (ë˜ëŠ” í˜„ì¬ í´ë”ì˜ base.py)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from base import AsyncBaseScraper

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class CBREScraper(AsyncBaseScraper):
    def __init__(self, start_date=None, end_date=None):
        super().__init__(start_date, end_date)
        self.site_name = "CBRE"

    async def scrape(self):
        """
        CBRE ì¸ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ë©”ì¸ ë¡œì§
        """
        async with self._create_context() as context:
            collected = 0
            logger.info(f"[{self.site_name}] ìˆ˜ì§‘ ì‹œì‘...")
            
            try:
                page = await context.new_page()
                # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ì†
                url = "https://www.cbrekorea.com/insights"
                await page.goto(url, wait_until='networkidle', timeout=30000)
                await page.wait_for_timeout(3000)
                
                # 2. ë¦¬í¬íŠ¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ (/insights/reports/ íŒ¨í„´)
                anchors = await page.query_selector_all("a[href*='/insights/reports/']")
                urls = []
                for a in anchors:
                    href = await a.get_attribute("href")
                    if href and href not in urls:
                        urls.append(urljoin(url, href))
                
                logger.info(f"   ğŸ” ìƒì„¸ ë§í¬ {len(urls)}ê°œ ë°œê²¬")
                
                # 3. ê° ìƒì„¸ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
                for detail_url in urls[:20]: # ìƒìœ„ 20ê°œ ì‹œë„
                    try:
                        logger.info(f"      ğŸ“– ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {detail_url}")
                        await page.goto(detail_url, wait_until='networkidle', timeout=20000)
                        await page.wait_for_timeout(1000)
                        
                        # ì œëª© ì¶”ì¶œ: h1
                        title = await page.inner_text("h1")
                        title = title.strip() if title else "No Title"
                        
                        # ë‚ ì§œ ì¶”ì¶œ: ë³¸ë¬¸ì—ì„œ 20XX íŒ¨í„´ íƒìƒ‰
                        date_text = "0000-00-00"
                        body_text = await page.inner_text("body")
                        date_match = re.search(r'20\d{2}[.-]\d{1,2}[.-]\d{1,2}', body_text)
                        if date_match:
                            date_text = date_match.group(0).replace('.', '-')
                        
                        # ê¸°ê°„ í•„í„°ë§
                        if date_text != "0000-00-00" and not self.is_in_period(date_text):
                            logger.info(f"         â›” ê¸°ê°„ ì œì™¸: {date_text}")
                            continue

                        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸° (a.cbre-c-download)
                        dl_button = await page.query_selector("a.cbre-c-download")
                        if dl_button:
                            pdf_url = await dl_button.get_attribute("href")
                            if pdf_url:
                                pdf_url = urljoin(detail_url, pdf_url.strip())
                                
                                # ê²°ê³¼ ì €ì¥
                                self.save_result(title, date_text, pdf_url, detail_url)
                                collected += 1
                                logger.info(f"         âœ… ìˆ˜ì§‘ ì„±ê³µ: {title[:20]}... ({date_text})")
                            else:
                                logger.warning(f"         âš ï¸ ë‹¤ìš´ë¡œë“œ ë§í¬ href ì—†ìŒ")
                        else:
                            logger.warning(f"         âš ï¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ë¯¸ë°œê²¬")
                            
                    except Exception as e:
                        logger.error(f"         âŒ ìƒì„¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

            except Exception as e:
                logger.error(f"   âŒ {self.site_name} ì „ì²´ ì˜¤ë¥˜: {e}")
            finally:
                await page.close()
                
            return collected

if __name__ == "__main__":
    # Windows í•œê¸€ ì¸ì½”ë”© ì„¤ì •
    if sys.platform == "win32":
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print(f"\n[ {CBREScraper().site_name} ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ]")
        try:
            default_start = "2024-01-01"
            start_in = input(f"ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: {default_start}]: ").strip()
            start_date = start_in if start_in else default_start
            
            end_in = input(f"ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: ì˜¤ëŠ˜]: ").strip()
            end_date = end_in if end_in else datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            sys.exit(0)
    
    scraper = CBREScraper(start_date, end_date)
    asyncio.run(scraper.run())
