"""
í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ ìŠ¤í¬ë˜í¼ (Playwright ë²„ì „)

ìˆ˜ì§‘ ëŒ€ìƒ:
1. ì—°êµ¬ë³´ê³ ì„œ (MN1000)
2. í•˜ë‚˜ê¸ˆìœµí¬ì»¤ìŠ¤ (MN2000) - ì‚¬ìš©ì ìš”ì²­ ì¶”ê°€

íŠ¹ì§•: 
- onclick ì´ë²¤íŠ¸(downloadItem) íŒŒì‹±í•˜ì—¬ PDF ë§í¬ ìƒì„±
- 2ê°œ ê²Œì‹œíŒ ìˆœì°¨ ìˆ˜ì§‘
"""

import sys
import os

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€ (base ëª¨ë“ˆ import ìœ„í•¨)
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from base import AsyncBaseScraper
import logging
import re
import asyncio
from urllib.parse import urljoin
from playwright.async_api import Page

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("HanaIfScraper")

class HanaIfScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        # ìƒì†ë°›ì€ í´ë˜ìŠ¤ì˜ ìƒì„±ì í˜¸ì¶œ ì‹œ site_name ì „ë‹¬
        super().__init__(start_date, end_date, site_name="í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ")
        self.site_name = "í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ"
        self.base_url = "https://www.hanaif.re.kr"

    def is_before_start_date(self, target_date_str: str) -> bool:
        """ë‚ ì§œ ë¹„êµ í—¬í¼: íƒ€ê²Ÿ ë‚ ì§œê°€ ì‹œì‘ì¼ë³´ë‹¤ ì´ì „ì¸ì§€ í™•ì¸ (YYYY-MM-DD)"""
        # self.start_dateê°€ datetime.date ê°ì²´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
        if not target_date_str or not self.start_date:
            return False
        return target_date_str < str(self.start_date)
        
    async def scrape(self, page: Page) -> int:
        total_collected = 0
        
        # ìˆ˜ì§‘í•  íƒ€ê²Ÿ ë¦¬ìŠ¤íŠ¸ (ìˆœì°¨ ì‹¤í–‰)
        targets = [
            {
                "name": "ì—°êµ¬ë³´ê³ ì„œ",
                "url": f"{self.base_url}/boardList.do?menuId=MN1000&tabMenuId=N"
            },
            {
                "name": "í•˜ë‚˜ê¸ˆìœµí¬ì»¤ìŠ¤",
                "url": f"{self.base_url}/boardList.do?menuId=MN2000&tabMenuId=MN2100"
            }
        ]
        
        for target in targets:
            logger.info(f"==================================================")
            logger.info(f"   [{self.site_name}] '{target['name']}' ìˆ˜ì§‘ ì‹œì‘")
            logger.info(f"   URL: {target['url']}")
            logger.info(f"==================================================")
            
            try:
                count = await self._scrape_board(page, target['url'], target['name'])
                total_collected += count
            except Exception as e:
                logger.error(f"   âŒ {target['name']} ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        
        return total_collected

    async def _scrape_board(self, page: Page, url: str, board_name: str) -> int:
        """ê°œë³„ ê²Œì‹œíŒ ìˆ˜ì§‘ ë¡œì§"""
        await page.goto(url, wait_until='networkidle', timeout=30000)
        await page.wait_for_timeout(3000) # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        collected_count = 0
        current_page = 1
        max_pages = 10 
        
        while current_page <= max_pages:
            logger.info(f"   ğŸ“„ í˜ì´ì§€ {current_page} ì½ëŠ” ì¤‘...")
            
            # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
            items = await page.query_selector_all('ul.listType01 > li')
            if not items:
                items = await page.query_selector_all('.board_list > li, .list_box > li, tbody > tr')
            
            if not items:
                logger.warning("   âš ï¸ ê²Œì‹œê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±)")
                # ë””ë²„ê¹…ìš©: HTML ì¼ë¶€ ì¶œë ¥
                # html = await page.content()
                # logger.debug(f"HTML Preview: {html[:500]}")
                break
                
            logger.info(f"   â†’ ì•„ì´í…œ {len(items)}ê°œ ë°œê²¬")
                
            page_collected = 0
            stop_signal = False
            
            for item in items:
                try:
                    # 1. ë‚ ì§œ ì¶”ì¶œ ë° ê¸°ê°„ ê²€ì¦
                    date_ele = await item.query_selector('.date')
                    if not date_ele: 
                        print("      [DEBUG] [Skip] ë‚ ì§œ ìš”ì†Œ ì—†ìŒ")
                        continue
                    
                    date_text = (await date_ele.text_content()).strip().replace('.', '-')
                    
                    # ê¸°ê°„ ì²´í¬
                    if self.is_before_start_date(date_text):
                        print(f"      [DEBUG] [Stop] ë‚ ì§œ ì§€ë‚¨: {date_text}")
                        stop_signal = True
                        break
                        
                    if not self.is_in_period(date_text):
                        print(f"      [DEBUG] [Skip] ê¸°ê°„ ë°– ë°ì´í„°: {date_text}")
                        continue
                        
                    # 2. ì œëª© ì¶”ì¶œ
                    title = "ì œëª© ì—†ìŒ"
                    # .hiddenEllips ë˜ëŠ” .tit ì‚¬ìš©
                    hidden = await item.query_selector('.hiddenEllips')
                    if hidden:
                        title = await hidden.text_content()
                    else:
                        tit_ele = await item.query_selector('.tit')
                        if tit_ele:
                            title = await tit_ele.text_content()
                    title = title.strip()
                    print(f"      [DEBUG] ì œëª© ì¶”ì¶œ: {title} ({date_text})")

                    # 3. ìƒì„¸ URL (í˜„ì¬ í˜ì´ì§€ URL ì‚¬ìš©)
                    full_url = url
                    
                    # 4. ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ (í•µì‹¬)
                    download_url = "N/A"
                    # ì„ íƒì í™•ì¥: .fileBox a, ë˜ëŠ” .file a, ë˜ëŠ” onclickì´ ìˆëŠ” ì•„ë¬´ a íƒœê·¸
                    file_btn = await item.query_selector('.fileBox a[onclick*="downloadItem"], .file a, a[onclick*="downloadItem"]')
                    
                    if file_btn:
                        onclick_text = await file_btn.get_attribute('onclick')
                        # downloadItem('36432', '102714') íŒ¨í„´ íŒŒì‹±
                        m = re.search(r"downloadItem\(\s*['\"]?(\d+)['\"]?,\s*['\"]?(\d+)['\"]?\s*\)", onclick_text)
                        if m:
                            seq = m.group(2) 
                            download_url = f"{self.base_url}/dev/hanaifFileDownload.jsp?seq={seq}"
                        else:
                            # seq í•˜ë‚˜ë§Œ ìˆëŠ” ê²½ìš°
                            m2 = re.search(r"downloadItem\(\s*['\"]?(\d+)['\"]?\s*\)", onclick_text)
                            if m2:
                                seq = m2.group(1)
                                download_url = f"{self.base_url}/dev/hanaifFileDownload.jsp?seq={seq}"
                    else:
                        print("      [DEBUG] [Skip] ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì—†ìŒ")
                    
                    if download_url != "N/A":
                        self.save_result(title, date_text, download_url, full_url)
                        logger.info(f"      âœ… ìˆ˜ì§‘: {title[:20]}... ({date_text})")
                        collected_count += 1
                        page_collected += 1
                    else:
                        logger.warning(f"      âš ï¸ ë‹¤ìš´ë¡œë“œ URL ì‹¤íŒ¨: {title}")
                    
                except Exception as e:
                    logger.warning(f"      âš ï¸ ì•„ì´í…œ ì²˜ë¦¬ ì—ëŸ¬: {e}")
            
            if stop_signal:
                logger.info("   ğŸ›‘ ì‹œì‘ì¼ ì´ì „ ë°ì´í„° ë°œê²¬ - ìˆ˜ì§‘ ì¢…ë£Œ")
                break
                
            if page_collected == 0 and current_page > 1:
                # ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ê³  ì²« í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ ì¢…ë£Œ (ë¹ˆ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)
                if len(items) == 0:
                    break
            
            # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
            current_page += 1
            # í˜ì´ì§€ ì´ë™ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰: goPage(2);
            # ë²„íŠ¼ ì°¾ê¸°: <div class="paging"> ... <a href="javascript:goPage(2);">...</a>
            next_btn = await page.query_selector(f'.paging a[href*="goPage({current_page})"]')
            
            if next_btn:
                await next_btn.click()
                await page.wait_for_timeout(1500) # ë¡œë”© ëŒ€ê¸°
            else:
                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                logger.info("   ğŸš« ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì—†ìŒ - ì¢…ë£Œ")
                break
                
        return collected_count

# ì‹¤í–‰ ë¸”ë¡
if __name__ == "__main__":
    import sys
    import asyncio
    
    # ìœˆë„ìš° ì¸ì½”ë”© ì„¤ì •
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except:
            pass
    
    start_date = None
    end_date = None
    
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        print("\n[í•˜ë‚˜ê¸ˆìœµì—°êµ¬ì†Œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            start_date = input("ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD): ").strip()
            if not start_date: start_date = "2024-01-01"
            
            end_date = input("ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD): ").strip()
            if not end_date: 
                import datetime
                end_date = datetime.datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            sys.exit(0)
            
    print(f"\nğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
    
    scraper = HanaIfScraper(start_date, end_date)
    
    # scrape_all()ì€ base.pyì— ì—†ìœ¼ë¯€ë¡œ (ì´ì „ ì½”ë“œ ì°¸ì¡°), 
    # ì§ì ‘ ë¸Œë¼ìš°ì € ë„ìš°ê³  scrape í˜¸ì¶œí•˜ëŠ” ë¡œì§ êµ¬í˜„
    async def run_standalone():
        from playwright.async_api import async_playwright
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True) # ë””ë²„ê¹… ì‹œ False
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            page = await context.new_page()
            await scraper.scrape(page)
            await browser.close()
            
    try:
        asyncio.run(run_standalone())
    except Exception as e:
        print(f"ì‹¤í–‰ ì˜¤ë¥˜: {e}")
