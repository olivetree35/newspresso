#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Group 3 í†µí•© ë°ì´í„° ìˆ˜ì§‘ê¸°
- ìœ„ì¹˜: scrapers_group3/success/
- ëŒ€ìƒ: KRIHS(12), KDI(13), CERIK(14), HRI(15)
"""

import sys
import os
import asyncio
import logging
import json
from datetime import datetime
import importlib.util

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ (scrapers_group3/success/)
current_dir = os.path.dirname(os.path.abspath(__file__))
# sys.pathì— í˜„ì¬ ë””ë ‰í† ë¦¬ ì¶”ê°€ (base.py ì„í¬íŠ¸ ë“±ì„ ìœ„í•´)
if current_dir not in sys.path:
    sys.path.append(current_dir)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("Collector")

def load_module(name, filename):
    path = os.path.join(current_dir, filename)
    if not os.path.exists(path):
        logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
        return None
        
    try:
        spec = importlib.util.spec_from_file_location(name, path)
        if spec is None:
            logger.error(f"âŒ ëª¨ë“ˆ ìŠ¤í™ ë¡œë“œ ì‹¤íŒ¨: {name}")
            return None
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
        return mod
    except Exception as e:
        logger.error(f"âŒ ëª¨ë“ˆ ë¡œë”© ì¤‘ ì˜ˆì™¸ ë°œìƒ ({name}): {e}")
        return None

class NewsCollectorGroup3:
    def __init__(self, start_date=None, end_date=None):
        self.start_date = start_date or "2025-01-01"
        self.end_date = end_date or datetime.now().strftime("%Y-%m-%d")
        self.output_dir = os.path.join(current_dir, "output")
        os.makedirs(self.output_dir, exist_ok=True)
        self.all_results = []

    async def run(self):
        logger.info("ğŸš€ Group 3 ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (í†µí•©ë³¸) ì‹œì‘")
        logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {self.start_date} ~ {self.end_date}")
        
        # ëª¨ë“ˆ ë¡œë“œ (ìˆœì„œ: KDI -> KRIHS -> CERIK -> HRI)
        modules = [
            ("KDI", "4_kdi_scraper.py", "KDIScraper"),
            ("KRIHS", "11_krihs_scraper.py", "KRIHSScraper"),
            ("CERIK", "13_cerik_scraper.py", "CERIKScraper"),
            ("HRI", "14_hri_scraper.py", "HRIScraper")
        ]
        
        for name, filename, class_name in modules:
            mod = load_module(name.lower(), filename)
            if not mod:
                continue
                
            try:
                ScraperClass = getattr(mod, class_name)
                scraper = ScraperClass(self.start_date, self.end_date)
                
                logger.info(f"\nâ–¶ {name} ìˆ˜ì§‘ ì‹œì‘... ({filename})")
                
                if hasattr(scraper, 'scrape_all'):
                     await scraper.scrape_all()
                elif hasattr(scraper, 'scrape'):
                     await scraper.scrape()
                
                # ê²°ê³¼ ìˆ˜ì§‘
                if hasattr(scraper, 'results'):
                    count = len(scraper.results)
                    self.all_results.extend(scraper.results)
                    logger.info(f"   âœ… {name}: {count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

        self.save_integrated_results()
        logger.info("\nâœ¨ ëª¨ë“  ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ!")

    def save_integrated_results(self):
        if not self.all_results:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì¤‘ë³µ ì œê±° (download_url ê¸°ì¤€)
        seen = set()
        unique_data = []
        for item in self.all_results:
            url = item.get('download_url')
            if url and url not in seen:
                seen.add(url)
                unique_data.append(item)
            elif not url: # URL ì—†ëŠ” ê²½ìš°ë„ í¬í•¨ (ì—ëŸ¬ ë¡œê·¸ìš© ë“±)
                unique_data.append(item)
                
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"group3_integrated_results_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=4)
            
        logger.info(f"ğŸ’¾ í†µí•© ê²°ê³¼ ì €ì¥: {filepath} ({len(unique_data)}ê±´)")

if __name__ == "__main__":
    if sys.platform == 'win32':
        try: sys.stdout.reconfigure(encoding='utf-8')
        except: pass

    s_date = None
    e_date = None
    
    if len(sys.argv) >= 3:
        s_date = sys.argv[1]
        e_date = sys.argv[2]
    else:
        print("\n" + "="*50)
        print("Group 3 í†µí•© ë°ì´í„° ìˆ˜ì§‘ê¸°")
        print("="*50)
        try:
            today = datetime.now().strftime("%Y-%m-%d")
            start_in = input("ì‹œì‘ì¼ (YYYY-MM-DD) [ê¸°ë³¸: 2024-01-01]: ").strip()
            s_date = start_in if start_in else "2024-01-01"
            
            end_in = input(f"ì¢…ë£Œì¼ (YYYY-MM-DD) [ê¸°ë³¸: {today}]: ").strip()
            e_date = end_in if end_in else today
        except KeyboardInterrupt:
            sys.exit(0)

    collector = NewsCollectorGroup3(s_date, e_date)
    asyncio.run(collector.run())
