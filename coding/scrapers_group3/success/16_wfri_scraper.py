#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìš°ë¦¬ê¸ˆìœµì—°êµ¬ì†Œ (WFRI) ìŠ¤í¬ë˜í¼
- URL: https://www.wfri.re.kr/ko/web/research_report/research_report.php?search_type=list
- êµ¬ì¡°: ëª©ë¡ -> ìƒì„¸ í˜ì´ì§€ -> ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Javascript onclick)
- ë‹¤ìš´ë¡œë“œ ë¡œì§: board_file_download('idx', 'board_code', 'file_cnt') íŒŒì‹± -> URL ì¡°ë¦½
"""

import sys
import os
import asyncio
import logging
import re
from datetime import datetime
from urllib.parse import urljoin
from playwright.async_api import async_playwright

# ìƒìœ„ í´ë”(success)ì˜ ë¶€ëª¨(scrapers_group3)ì—ì„œ base.pyë¥¼ ì°¾ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
cur_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(cur_dir) # scrapers_group3
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

try:
    from success.base import AsyncBaseScraper
except ImportError:
    # run_standalone ë“±ì˜ ìƒí™© ê³ ë ¤
    sys.path.append(cur_dir)
    try:
        from base import AsyncBaseScraper
    except ImportError:
        print("âŒ base.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("WFRIScraper")

class WFRIScraper(AsyncBaseScraper):
    def __init__(self, start_date, end_date):
        super().__init__(start_date, end_date, site_name="ìš°ë¦¬ê¸ˆìœµì—°êµ¬ì†Œ")
        self.base_url = "https://www.wfri.re.kr"
        # ì´ˆê¸° ì§„ì… URL
        self.target_url = "https://www.wfri.re.kr/ko/web/research_report/research_report.php?search_type=list"

    async def _scrape_board(self, page, url):
        """ê²Œì‹œíŒ ìˆ˜ì§‘ (ëª©ë¡ -> ìƒì„¸ ì§„ì… ë°©ì‹)"""
        await page.goto(url, wait_until='networkidle', timeout=30000)
        await page.wait_for_timeout(2000)
        
        collected_count = 0
        current_page = 1
        max_pages = 10 
        
        while current_page <= max_pages:
            logger.info(f"   ğŸ“„ í˜ì´ì§€ {current_page} ì½ëŠ” ì¤‘...")
            
            # ëª©ë¡ ì•„ì´í…œ ì¶”ì¶œ (í…Œì´ë¸” í–‰ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ)
            # êµ¬ì¡° ì¶”ì •: .tbl-list tbody tr ë˜ëŠ” ìœ ì‚¬ êµ¬ì¡°
            items = await page.query_selector_all('tbody > tr, li.item, .list_box > li')
            
            if not items:
                logger.warning("   âš ï¸ ëª©ë¡ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
                
            logger.info(f"   â†’ ì•„ì´í…œ {len(items)}ê°œ ë°œê²¬")
            
            page_collected = 0
            
            # ìƒì„¸ í˜ì´ì§€ ì´ë™ì„ ìœ„í•´ ë§í¬ ìš”ì†Œë“¤ì„ ë¨¼ì € ìˆ˜ì§‘ (DOM ë³€ê²½ ë°©ì§€ ìœ„í•´ href/onclick ì •ë³´ ë“± ìˆ˜ì§‘ í•„ìš”í•˜ë‚˜, ìƒì„¸ ê°”ë‹¤ê°€ backí•˜ë©´ elementê°€ ê°±ì‹ ë¨)
            # ë”°ë¼ì„œ 'í•˜ë‚˜ì”©' ì²˜ë¦¬í•˜ê³  ëª©ë¡ìœ¼ë¡œ 'ëŒì•„ì˜¤ëŠ”' ë°©ì‹ ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
            
            # itemsëŠ” handleì´ë¯€ë¡œ, í˜ì´ì§€ë¥¼ ë²—ì–´ë‚˜ë©´ íš¨ë ¥ì„ ìƒì„ ìˆ˜ ìˆìŒ.
            # ë£¨í”„ë¥¼ ëŒ ë•Œ ë§¤ë²ˆ ëª©ë¡ì„ ë‹¤ì‹œ ì¡ê±°ë‚˜, nth-childë¡œ ì ‘ê·¼í•´ì•¼ í•¨.
            
            # ì „ëµ: í˜„ì¬ í˜ì´ì§€ì˜ ì•„ì´í…œ ê°œìˆ˜ë§Œí¼ ë°˜ë³µí•˜ë©° nthë¡œ ì ‘ê·¼
            item_count = len(items)
            
            for i in range(item_count):
                try:
                    # í˜ì´ì§€ê°€ ë¦¬ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ ì¿¼ë¦¬
                    current_items = await page.query_selector_all('tbody > tr, li.item, .list_box > li')
                    if i >= len(current_items):
                        break
                    
                    item = current_items[i]
                    
                    # 1. ë‚ ì§œ ì¶”ì¶œ (ëª©ë¡ì—ì„œ ë¨¼ì € í™•ì¸í•˜ì—¬ Skip ì—¬ë¶€ ê²°ì •)
                    date_text = "0000-00-00"
                    
                    # ë‚ ì§œ ì…€ë ‰í„° ì¶”ì • (.date, td.date, td:nth-child...)
                    date_ele = await item.query_selector('.date, td.date, span.date')
                    # ë§Œì•½ classê°€ ì—†ë‹¤ë©´ <td> ì¤‘ ë‚ ì§œ í˜•ì‹ì´ ìˆëŠ” ê²ƒ ì°¾ê¸°
                    if not date_ele:
                        tds = await item.query_selector_all('td')
                        for td in tds:
                            txt = (await td.text_content()).strip()
                            if re.match(r'\d{4}[.-]\d{2}[.-]\d{2}', txt):
                                date_text = txt
                                break
                    else:
                        date_text = (await date_ele.text_content()).strip()
                        
                    date_text = date_text.replace('.', '-')
                    
                    # ê¸°ê°„ ì²´í¬
                    if not self.is_in_period(date_text):
                        if date_text != "0000-00-00" and date_text < str(self.start_date):
                             # ë‚ ì§œìˆœ ì •ë ¬ì´ë¼ê³  ê°€ì •í•˜ê³ , ì‹œì‘ì¼ë³´ë‹¤ ì´ì „ì´ë©´ ì¤‘ë‹¨(ì˜µì…˜)
                             # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ continue
                             pass
                        # continue # ìƒì„¸ ì§„ì… ì „ì— ë‚ ì§œë¡œ ê±°ë¦„ (íš¨ìœ¨ì„±)
                        # ë‚ ì§œê°€ ì—†ìœ¼ë©´(0000-00-00) ì¼ë‹¨ ìƒì„¸ ë“¤ì–´ê°€ë³¼ ìˆ˜ë„ ìˆìŒ. ì¼ë‹¨ì€ ìŠ¤í‚µ ì•ˆí•¨.
                    
                    if date_text != "0000-00-00" and not self.is_in_period(date_text):
                        logger.debug(f"      [Skip] ê¸°ê°„ ë°–: {date_text}")
                        continue

                    # 2. ì œëª© ìš”ì†Œ ì°¾ê¸°
                    title_ele = await item.query_selector('a.tbl-link, .title a, a')
                    if not title_ele:
                        continue
                        
                    title = (await title_ele.text_content()).strip()
                    logger.debug(f"   [{i+1}/{item_count}] ë¶„ì„: {title} ({date_text})")
                    
                    # 3. ìƒì„¸ í˜ì´ì§€ ì§„ì…
                    # í´ë¦­ ì‹œ í˜ì´ì§€ ì´ë™ ë°œìƒ
                    
                    # í´ë¦­ ì „ hrefë‚˜ onclick í™•ì¸
                    # hrefê°€ ìˆìœ¼ë©´ ìƒˆ íƒ­ìœ¼ë¡œ ì—¬ëŠ”ê²Œ ë¹ ë¦„ (ë’¤ë¡œê°€ê¸°ë³´ë‹¤)
                    # ì‚¬ìš©ì ì •ë³´: onclick="ajax_board_view_count('2568');" -> href="..." ë„ ê°™ì´ ìˆì„ê²ƒì„.
                    
                    # ìƒˆ íƒ­ ì—´ê¸° ì‹œë„ (Ctrl+Click)
                    # modifier key ì‚¬ìš©ì´ ì§€ì›ë¨
                    
                    # í•˜ì§€ë§Œ hrefê°€ 'javascript:...' í˜•ì‹ì´ë©´ ìƒˆ íƒ­ì´ ì•ˆ ì—´ë¦´ ìˆ˜ ìˆìŒ.
                    # ì¼ë‹¨ í´ë¦­í•˜ê³  ëŒì•„ì˜¤ëŠ” ë°©ì‹(go_back) ì‚¬ìš©
                    
                    await title_ele.click()
                    await page.wait_for_load_state('networkidle', timeout=10000)
                    
                    # --- ìƒì„¸ í˜ì´ì§€ ---
                    
                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ë‹¤ì‹œ í™•ì¸ (ëª©ë¡ì— ì—†ì—ˆì„ ê²½ìš°)
                    if date_text == "0000-00-00":
                        date_detail = await page.query_selector('.view-date, .date, .info')
                        if date_detail:
                            txt = await date_detail.text_content()
                            # ì •ê·œì‹ìœ¼ë¡œ YYYY-MM-DD ì¶”ì¶œ
                            m = re.search(r'\d{4}[.-]\d{2}[.-]\d{2}', txt)
                            if m:
                                date_text = m.group(0).replace('.', '-')
                    
                    # ê¸°ê°„ ì¬ê²€ì¦
                    if not self.is_in_period(date_text):
                        logger.debug("      [Skip-Detail] ê¸°ê°„ ë°–")
                        await page.go_back()
                        await page.wait_for_load_state('networkidle')
                        continue

                    # 4. ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ
                    # ìš”ì†Œ: <a href="javascript:void(0);" onclick="board_file_download('2568','research_report','2')">
                    
                    doc_url = "N/A"
                    # onclickì— board_file_downloadê°€ ìˆëŠ” a íƒœê·¸ ì°¾ê¸°
                    down_btn = await page.query_selector('a[onclick*="board_file_download"]')
                    
                    if down_btn:
                        onclick_val = await down_btn.get_attribute('onclick')
                        # íŒŒì‹±: board_file_download('2568','research_report','2')
                        # í™‘ë”°ì˜´í‘œ ë˜ëŠ” ìŒë”°ì˜´í‘œ, ê³µë°± ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
                        m = re.search(r"board_file_download\(\s*['\"]?(.+?)['\"]?,\s*['\"]?(.+?)['\"]?,\s*['\"]?(.+?)['\"]?\s*\)", onclick_val)
                        if m:
                            idx = m.group(1)
                            board_code = m.group(2)
                            file_cnt = m.group(3)
                            
                            # URL ì¡°ë¦½
                            # https://www.wfri.re.kr/module/lib/board_file_download.php?idx=2568&board_code=research_report&file_cnt=2
                            doc_url = f"{self.base_url}/module/lib/board_file_download.php?idx={idx}&board_code={board_code}&file_cnt={file_cnt}"
                        else:
                            logger.warning(f"      íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {onclick_val}")
                    
                    if doc_url != "N/A":
                        logger.info(f"      âœ… ìˆ˜ì§‘ ì„±ê³µ: {title[:20]}... ({date_text})")
                        self.save_result(title, date_text, doc_url, page.url)
                        collected_count += 1
                        page_collected += 1
                    else:
                        logger.warning(f"      âš ï¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì—†ìŒ: {title}")

                    # ëª©ë¡ìœ¼ë¡œ ë³µê·€
                    await page.go_back()
                    await page.wait_for_load_state('networkidle')
                    
                except Exception as e:
                    logger.error(f"      âŒ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # í˜¹ì‹œ ìƒì„¸í˜ì´ì§€ì— ê°‡í˜”ìœ¼ë©´ ë³µê·€ ì‹œë„
                    if "research_report.php" not in page.url or "view" in page.url:
                        try:
                            await page.go_back()
                            await page.wait_for_load_state('networkidle')
                        except:
                            pass
            
            # í˜ì´ì§€ë„¤ì´ì…˜ (ë‹¤ìŒ í˜ì´ì§€)
            # <div class="paging"> ... <a href="..." class="next"></a>
            next_btn = await page.query_selector('.paging .next, .btn_next')
            if next_btn:
                logger.info("   â–¶ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
                await next_btn.click()
                await page.wait_for_load_state('networkidle')
                current_page += 1
            else:
                logger.info("   ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€")
                break
                
        return collected_count

    async def scrape(self):
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 720}
            )
            page = await context.new_page()
            
            logger.info(f"ğŸš€ [ìš°ë¦¬ê¸ˆìœµì—°êµ¬ì†Œ] ìˆ˜ì§‘ ì‹œì‘ ({self.start_date} ~ {self.end_date})")
            
            try:
                await self._scrape_board(page, self.target_url)
            except Exception as e:
                logger.error(f"âŒ ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            finally:
                await browser.close()
                

if __name__ == "__main__":
    # ìœˆë„ìš° ì¸ì½”ë”© ì„¤ì •
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except:
            pass

    import sys
    
    # ê¸°ë³¸ê°’
    start_date = None
    end_date = None
    
    # 1. ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
    else:
        # 2. ëŒ€í™”í˜• ì…ë ¥
        print("\n[ìš°ë¦¬ê¸ˆìœµì—°êµ¬ì†Œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰]")
        try:
            input_start = input("ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD) [ì—”í„°: 2024-01-01]: ").strip()
            if input_start:
                start_date = input_start
            else:
                start_date = "2024-01-01"
                
            input_end = input(f"ìˆ˜ì§‘ ì¢…ë£Œì¼ (YYYY-MM-DD) [ì—”í„°: ì˜¤ëŠ˜]: ").strip()
            if input_end:
                end_date = input_end
            else:
                end_date = datetime.now().strftime("%Y-%m-%d")
        except KeyboardInterrupt:
            print("\nì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)
            
    # ì‹¤í–‰
    scraper = WFRIScraper(start_date, end_date)
    asyncio.run(scraper.scrape())
    
    # ê²°ê³¼ ì €ì¥ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
    if scraper.results:
        import json
        output_dir = os.path.join(cur_dir, "output")
        os.makedirs(output_dir, exist_ok=True)
        filename = f"wfri_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(scraper.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
