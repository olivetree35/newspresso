#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Group 3 í†µí•© ë°ì´í„° ìˆ˜ì§‘ê¸°
- ëŒ€ìƒ ì‚¬ì´íŠ¸: KRIHS(êµ­í† ì—°êµ¬ì›), KDI(í•œêµ­ê°œë°œì—°êµ¬ì›), CERIK(í•œêµ­ê±´ì„¤ì‚°ì—…ì—°êµ¬ì›), HRI(í˜„ëŒ€ê²½ì œì—°êµ¬ì›)
- ê¸°ëŠ¥: ê° ì „ìš© ìŠ¤í¬ë˜í¼ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  í•˜ë‚˜ì˜ ê²°ê³¼ë¡œ í†µí•©
"""

import sys
import os
import asyncio
import logging
import json
from datetime import datetime

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ (success/)
current_dir = os.path.dirname(os.path.abspath(__file__))
# ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ ê²½ë¡œ (success/scrapers_group3/)
scrapers_dir = os.path.join(current_dir, "scrapers_group3")

# sys.pathì— ì¶”ê°€
if scrapers_dir not in sys.path:
    sys.path.append(scrapers_dir)

# ë™ì ìœ¼ë¡œ ëª¨ë“ˆ ì„í¬íŠ¸
import importlib.util

def load_module(name, path):
    if not os.path.exists(path):
        # ë§Œì•½ scrapers_group3 í´ë”ê°€ ì´ì¤‘ìœ¼ë¡œ ìˆê±°ë‚˜ ê²½ë¡œê°€ ë‹¤ë¥´ë©´ ì°¾ì•„ë³´ê¸°
        # ì˜ˆ: success/scrapers_group3/success/scrapers_group3 ? (ì‚¬ìš©ìê°€ mvë¥¼ ì—¬ëŸ¬ë²ˆ í–ˆì„ ìˆ˜ë„ ìˆìŒ)
        # ì¼ë‹¨ ê¸°ë³¸ ê²½ë¡œ ì‹œë„
        pass
        
    spec = importlib.util.spec_from_file_location(name, path)
    if spec is None:
        raise ImportError(f"ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)
    return mod

# ê²½ë¡œ ì„¤ì • (íŒŒì¼ëª… í™•ì¸ í•„ìš”)
# KRIHS: 10_1_krihs_scraper.py
# KDI: kdi.py
# CERIK: 14_cerik_scraper.py
# HRI: 15_hri_scraper.py

try:
    krihs_path = os.path.join(scrapers_dir, "10_1_krihs_scraper.py")
    if not os.path.exists(krihs_path): # íŒŒì¼ëª…ì´ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„
         krihs_path = os.path.join(scrapers_dir, "12_krihs_scraper.py")
         
    krihs_mod = load_module("krihs", krihs_path)
    kdi_mod = load_module("kdi", os.path.join(scrapers_dir, "kdi.py"))
    cerik_mod = load_module("cerik", os.path.join(scrapers_dir, "14_cerik_scraper.py"))
    hri_mod = load_module("hri", os.path.join(scrapers_dir, "15_hri_scraper.py"))

    KRIHSScraper = krihs_mod.KRIHSScraper
    KDIScraper = kdi_mod.KDIScraper
    CERIKScraper = cerik_mod.CERIKScraper
    HRIScraper = hri_mod.HRIScraper

except Exception as e:
    print(f"âŒ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
    # ë””ë ‰í† ë¦¬ ëª©ë¡ ì¶œë ¥í•´ì„œ ë””ë²„ê¹…
    print(f"ğŸ“‚ {scrapers_dir} ëª©ë¡:")
    try:
        for f in os.listdir(scrapers_dir):
            print(f" - {f}")
    except:
        print(" (ë””ë ‰í† ë¦¬ë¥¼ ì½ì„ ìˆ˜ ì—†ìŒ)")
    sys.exit(1)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("Collector")

class NewsCollectorGroup3:
    def __init__(self, start_date=None, end_date=None):
        self.start_date = start_date or "2025-01-01"
        self.end_date = end_date or datetime.now().strftime("%Y-%m-%d")
        self.output_dir = os.path.join(current_dir, "output")
        os.makedirs(self.output_dir, exist_ok=True)
        self.all_results = []

    async def run(self):
        logger.info("ğŸš€ Group 3 ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘")
        logger.info(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {self.start_date} ~ {self.end_date}")
        
        scrapers = [
            ("KRIHS (êµ­í† ì—°êµ¬ì›)", KRIHSScraper(self.start_date, self.end_date)),
            ("KDI (í•œêµ­ê°œë°œì—°êµ¬ì›)", KDIScraper(self.start_date, self.end_date)),
            ("CERIK (í•œêµ­ê±´ì„¤ì‚°ì—…ì—°êµ¬ì›)", CERIKScraper(self.start_date, self.end_date)),
            ("HRI (í˜„ëŒ€ê²½ì œì—°êµ¬ì›)", HRIScraper(self.start_date, self.end_date))
        ]
        
        for name, scraper in scrapers:
            try:
                logger.info(f"\nâ–¶ {name} ìˆ˜ì§‘ ì‹œì‘...")
                if hasattr(scraper, 'scrape_all'):
                     await scraper.scrape_all()
                elif hasattr(scraper, 'scrape'):
                     await scraper.scrape()
                else:
                    logger.error(f"âŒ {name}: ì‹¤í–‰ ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    continue
                
                # ê²°ê³¼ ìˆ˜ì§‘
                if hasattr(scraper, 'results'):
                    self.all_results.extend(scraper.results)
                    logger.info(f"   âœ… {len(scraper.results)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ {name} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()

        self.save_integrated_results()
        logger.info("\nâœ¨ ëª¨ë“  ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ!")

    def save_integrated_results(self):
        if not self.all_results:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì¤‘ë³µ ì œê±° (download_url ê¸°ì¤€)
        seen = set()
        unique_data = []
        for item in self.all_results:
            url = item.get('download_url')
            if url and url not in seen:
                seen.add(url)
                unique_data.append(item)
                
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"group3_integrated_results_{timestamp}.json"
        
        # output í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± (ìƒì„±ìì—ì„œë„ í•˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ)
        os.makedirs(self.output_dir, exist_ok=True)
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=4)
            
        logger.info(f"ğŸ’¾ í†µí•© ê²°ê³¼ ì €ì¥: {filepath} ({len(unique_data)}ê±´)")

if __name__ == "__main__":
    if len(sys.argv) == 3:
        s_date, e_date = sys.argv[1], sys.argv[2]
    else:
        s_date = "2025-12-01"
        e_date = "2026-01-31"

    collector = NewsCollectorGroup3(s_date, e_date)
    asyncio.run(collector.run())
