#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KDI í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import os
import logging
import csv
from datetime import datetime
from playwright.async_api import async_playwright

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout.reconfigure(encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers_group3.kdi import KDIScraper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

async def test_kdi_scraper():
    """KDI ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì • (1ê°œì›” ë‹¨ìœ„ í…ŒìŠ¤íŠ¸)
    start_date = "2026-01-01"
    end_date = "2026-01-31"
    
    print(f"\n{'='*80}")
    print(f"ğŸ” KDI ì •ì±…ìë£Œì‹¤ í…ŒìŠ¤íŠ¸")
    print(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"{'='*80}\n")
    
    scraper = KDIScraper(start_date, end_date)
    
    async with async_playwright() as p:
        # ë´‡ ê°ì§€ ìš°íšŒ ì„¤ì • í¬í•¨
        browser = await p.chromium.launch(
            headless=False,  # Trueë¡œ ë³€ê²½í•˜ë©´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox'
            ]
        )
        
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='ko-KR',
            timezone_id='Asia/Seoul',
            extra_http_headers={
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            }
        )
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            window.chrome = { runtime: {} };
        """)
        
        page = await scraper._setup_page(context)
        
        try:
            collected = await scraper.scrape(page)
            
            print(f"\n{'='*80}")
            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {collected}ê±´")
            print(f"{'='*80}\n")
            
            # ê²°ê³¼ ì¶œë ¥
            if scraper.results:
                print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:\n")
                for i, item in enumerate(scraper.results[:10], 1):
                    print(f"{i}. {item['title'][:50]}")
                    print(f"   ë‚ ì§œ: {item['date']}")
                    print(f"   PDF: {item['pdf_url'][:80] if item['pdf_url'] != 'N/A' else '(ë¯¸ì¶”ì¶œ)'}\n")
                
                # CSV ì €ì¥
                output_dir = r"D:\Antigravity\coding\output"
                os.makedirs(output_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"kdi_{timestamp}.csv"
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=['source', 'title', 'date', 'page_url', 'pdf_url', 'collected_at'])
                    writer.writeheader()
                    writer.writerows(scraper.results)
                
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
            else:
                print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await context.close()
            await browser.close()

if __name__ == "__main__":
    asyncio.run(test_kdi_scraper())
