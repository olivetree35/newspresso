#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NICE ì‹ ìš©í‰ê°€ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import os
import logging
import csv
from datetime import datetime
from playwright.async_api import async_playwright

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout.reconfigure(encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers_group3.nice import NICEScraper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

async def test_nice_scraper():
    """NICE ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì • (1ê°œì›”)
    start_date = "2026-01-01"
    end_date = "2026-01-31"
    
    print(f"\n{'='*80}")
    print(f"ğŸ” NICE ì‹ ìš©í‰ê°€ í…ŒìŠ¤íŠ¸")
    print(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"{'='*80}\n")
    
    scraper = NICEScraper(start_date, end_date)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='ko-KR',
            timezone_id='Asia/Seoul'
        )
        
        page = await scraper._setup_page(context)
        
        try:
            collected = await scraper.scrape(page)
            
            print(f"\n{'='*80}")
            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {collected}ê±´")
            print(f"{'='*80}\n")
            
            # ê²°ê³¼ ì¶œë ¥
            # ê²°ê³¼ ì¶œë ¥ ë° ë‹¤ìš´ë¡œë“œ ê²€ì¦
            if scraper.results:
                print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ë° ë‹¤ìš´ë¡œë“œ ê²€ì¦:\n")
                
                # ê²€ì¦ì„ ìœ„í•œ í—¤ë” ì„¤ì •
                headers = {
                    "Referer": "https://www.nicerating.com/research/researchAll.do",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                }
                
                for i, item in enumerate(scraper.results[:10], 1):
                    print(f"{i}. {item['title'][:50]}")
                    print(f"   ë‚ ì§œ: {item['date']}")
                    pdf_url = item.get('pdf_url', 'N/A')
                    print(f"   PDF URL: {pdf_url[:80]}...")
                    
                    if pdf_url != 'N/A' and pdf_url.startswith('http'):
                        try:
                            # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (APIRequestContext ì‚¬ìš©)
                            response = await context.request.get(pdf_url, headers=headers)
                            status = response.status
                            content_type = response.headers.get('content-type', '')
                            content_disp = response.headers.get('content-disposition', '')
                            
                            if status == 200:
                                print(f"   âœ… [ê²€ì¦ ì„±ê³µ] ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥ (Status: 200)")
                                print(f"      Content-Type: {content_type}")
                            else:
                                print(f"   âŒ [ê²€ì¦ ì‹¤íŒ¨] Status: {status}, Type: {content_type}")
                            
                            # ì‘ë‹µ ë‹«ê¸° (ë©”ëª¨ë¦¬ í•´ì œ)
                            await response.dispose()
                                
                        except Exception as e:
                            print(f"   âš ï¸ [ê²€ì¦ ì—ëŸ¬] {e}")
                    print("")
                
                # CSV ì €ì¥
                output_dir = r"D:\Antigravity\coding\output"
                os.makedirs(output_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"nice_{timestamp}.csv"
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=['source', 'title', 'date', 'page_url', 'pdf_url', 'collected_at'])
                    writer.writeheader()
                    writer.writerows(scraper.results)
                
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
            else:
                print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await context.close()
            await browser.close()

if __name__ == "__main__":
    asyncio.run(test_nice_scraper())
