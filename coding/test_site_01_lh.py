#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Site 1 í…ŒìŠ¤íŠ¸: LHí† ì§€ì£¼íƒì—°êµ¬ì›
"""

import asyncio
import sys
import os
import logging
import csv
from datetime import datetime
from playwright.async_api import async_playwright

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout.reconfigure(encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers_group3.lh_ri import LHScraper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

async def test_lh_scraper():
    """LH ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì • (ìµœê·¼ 1ë…„)
    start_date = "2024-01-01"
    end_date = "2026-12-31"
    
    print(f"\n{'='*80}")
    print(f"ğŸ” Site 1 í…ŒìŠ¤íŠ¸: LHí† ì§€ì£¼íƒì—°êµ¬ì›")
    print(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"{'='*80}\n")
    
    scraper = LHScraper(start_date, end_date)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        
        page = await scraper._setup_page(context)
        
        try:
            collected = await scraper.scrape(page)
            
            print(f"\n{'='*80}")
            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {collected}ê±´")
            print(f"{'='*80}\n")
            
            # ê²°ê³¼ ì¶œë ¥
            if scraper.results:
                print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:\n")
                for i, item in enumerate(scraper.results[:10], 1):
                    print(f"{i}. {item['title'][:50]}")
                    print(f"   ë‚ ì§œ: {item['date']}")
                    print(f"   PDF: {item['pdf_url'][:80] if item['pdf_url'] != 'N/A' else '(ë¯¸ì¶”ì¶œ)'}\n")
                
                # CSV ì €ì¥
                output_dir = r"D:\Antigravity\coding\scrapers_group3\output"
                os.makedirs(output_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"site_01_lh_{timestamp}.csv"
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=['source', 'title', 'date', 'page_url', 'pdf_url', 'collected_at'])
                    writer.writeheader()
                    writer.writerows(scraper.results)
                
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
            else:
                print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await context.close()
            await browser.close()

if __name__ == "__main__":
    asyncio.run(test_lh_scraper())
