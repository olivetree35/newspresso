#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Group 3 í†µí•© ìˆ˜ì§‘ê¸° (Main Operator)
- CLI ì¸ì ì§€ì› (--site, --start, --end)
- ë¯¸ì…ë ¥ ì‹œ Interactive ëª¨ë“œ ì§€ì›
"""

import sys
import os
import asyncio
import logging
import argparse
import csv
from datetime import datetime

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout.reconfigure(encoding='utf-8')

# ë‚´ë¶€ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from playwright.async_api import async_playwright
# ê°œë³„ ìŠ¤í¬ë˜í¼ import
from scrapers_group3.lh_ri import LHScraper
from scrapers_group3.kif import KIFScraper
from scrapers_group3.nice import NICEScraper
from scrapers_group3.kdi import KDIScraper
from scrapers_group3.utils import save_to_csv

# ìŠ¤í¬ë˜í¼ ë“±ë¡ ë§µ
SCRAPERS = {
    'lh': LHScraper,
    'kif': KIFScraper,
    'nice': NICEScraper,
    'kdi': KDIScraper,
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("Main")

async def run_scraper(site_code: str, start_date: str, end_date: str):
    print(f"\nğŸš€ ìŠ¤í¬ë˜í¼ ì‹¤í–‰: {site_code.upper()} ({start_date} ~ {end_date})")
    
    if site_code not in SCRAPERS:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ ì½”ë“œì…ë‹ˆë‹¤: {site_code}")
        print(f"   ì§€ì› ëª©ë¡: {list(SCRAPERS.keys())}")
        return

    scraper_cls = SCRAPERS[site_code]
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ëŸ°ì¹­
        browser = await p.chromium.launch(headless=True)
        # ì¤‘ìš”: User-Agent ì„¤ì •
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        
        # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™” ë° ì‹¤í–‰
        # ê° ìŠ¤í¬ë˜í¼ëŠ” (start_date, end_date)ë¥¼ ì¸ìë¡œ ë°›ìŒ
        scraper = scraper_cls(start_date, end_date)
        
        # í˜ì´ì§€ ìƒì„± (ë„¤íŠ¸ì›Œí¬ ë¦¬ìŠ¤ë„ˆ ë“±ë¡ ë“± í¬í•¨ë  ìˆ˜ ìˆìŒ)
        # BaseScraperì˜ _on_responseë¥¼ ì“°ë ¤ë©´ page ìƒì„± ì‹œ hook í•„ìš”
        # í•˜ì§€ë§Œ í˜„ì¬ êµ¬í˜„ì€ scraper.scrape(page) ë‚´ë¶€ ë¡œì§ì— ì˜ì¡´í•˜ê±°ë‚˜
        # scraper._setup_page()ë¥¼ í˜¸ì¶œí•´ì•¼ í•¨.
        pass
        
        # AsyncBaseScraper êµ¬ì¡°ìƒ _setup_pageê°€ ìˆìœ¼ë¯€ë¡œ í™œìš©
        page = await scraper._setup_page(context)
        
        count = await scraper.scrape(page)
        
        await context.close()
        await browser.close()
        
        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {count}ê±´")
        
        # ê²°ê³¼ ì €ì¥ (CSV)
        save_to_csv(scraper.results, site_code)
        
        # ê²°ê³¼ ì¶œë ¥ (ê²€ì¦ìš©)
        if scraper.results:
            print("\nğŸ” [ìˆ˜ì§‘ ë°ì´í„° ìƒ˜í”Œ]")
            for item in scraper.results[:5]:
                print(f" - {item['date']} | {item['title'][:30]}... | PDF: {item['pdf_url']}")
                if item['pdf_url'] == 'N/A' or not item['pdf_url']:
                    print(f"   âš ï¸ PDF URL ëˆ„ë½ í™•ì¸ í•„ìš”: {item['page_url']}")

def main():
    parser = argparse.ArgumentParser(description='Group 3 Scraper Executor')
    parser.add_argument('--site', help='ì‚¬ì´íŠ¸ ì½”ë“œ (lh, kif, ...)')
    parser.add_argument('--start', help='ì‹œì‘ì¼ (YYYY-MM-DD)')
    parser.add_argument('--end', help='ì¢…ë£Œì¼ (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    # ì¸ìê°€ ì—†ìœ¼ë©´ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (ì—¬ê¸°ì„  ìƒëµí•˜ê³  CLI ìœ„ì£¼ë¡œ êµ¬í˜„)
    if not args.site or not args.start or not args.end:
        print("âŒ ì‚¬ìš©ë²•: python main_group3.py --site [code] --start [YYYY-MM-DD] --end [YYYY-MM-DD]")
        # í•„ìš” ì‹œ input() ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
        return

    asyncio.run(run_scraper(args.site, args.start, args.end))

if __name__ == "__main__":
    main()
