#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RICTON(ëŒ€í•œê±´ì„¤ì •ì±…ì—°êµ¬ì›) í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import sys
import os
import logging
import csv
from datetime import datetime
from playwright.async_api import async_playwright

# í•œê¸€ ì¶œë ¥ ì„¤ì •
sys.stdout.reconfigure(encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers_group3.ricon import RICONScraper

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

async def test_ricon_scraper():
    """RICON ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì • (1ê°œì›”)
    start_date = "2026-01-01"
    end_date = "2026-01-31"
    
    print(f"\n{'='*80}")
    print(f"ğŸ” ëŒ€í•œê±´ì„¤ì •ì±…ì—°êµ¬ì›(RICON) í…ŒìŠ¤íŠ¸")
    print(f"ğŸ“… ê¸°ê°„: {start_date} ~ {end_date}")
    print(f"{'='*80}\n")
    
    scraper = RICONScraper(start_date, end_date)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        )
        
        page = await scraper._setup_page(context)
        
        try:
            # 1. í˜ì´ì§€ DOM ë””ë²„ê¹… (ì„ íƒì í™•ì¸ìš©)
            await page.goto(scraper.url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(3000)
            
            print("\n[ë””ë²„ê¹…] í˜ì´ì§€ êµ¬ì¡° í™•ì¸:")
            
            # HTML ì €ì¥
            content = await page.content()
            with open("debug_ricon.html", "w", encoding="utf-8") as f:
                f.write(content)
            print("ğŸ’¾ HTML ë¤í”„ ì €ì¥: debug_ricon.html")
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            await page.screenshot(path="debug_ricon.png", full_page=True)
            print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_ricon.png")
            
            # ê²Œì‹œë¬¼ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ í™•ì¸
            containers = await page.query_selector_all('.board-list, .list-wrap, table.list')
            for c in containers:
                cls = await c.get_attribute('class')
                print(f"  - ë°œê²¬ëœ ì»¨í…Œì´ë„ˆ í´ë˜ìŠ¤: {cls}")
                
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            collected = await scraper.scrape(page)
            
            print(f"\n{'='*80}")
            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {collected}ê±´")
            print(f"{'='*80}\n")
            
            if scraper.results:
                print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:\n")
                for i, item in enumerate(scraper.results[:10], 1):
                    print(f"{i}. {item['title'][:50]}")
                    print(f"   ë‚ ì§œ: {item['date']}")
                    print(f"   PDF: {item['pdf_url'][:80] if item['pdf_url'] != 'N/A' else '(ë¯¸ì¶”ì¶œ)'}\n")
                
                # --- [ì¶”ê°€] URL ìœ íš¨ì„± ê²€ì¦ ---
                if len(scraper.results) > 0:
                    test_item = scraper.results[0]
                    test_url = test_item['pdf_url']
                    referer_url = test_item.get('page_url', scraper.base_url) # ìƒì„¸ í˜ì´ì§€ URL
                    
                    if test_url != "N/A":
                        print(f"\nğŸ§ª URL ìœ íš¨ì„± ê²€ì¦ ì‹œë„: {test_url}")
                        try:
                            # Playwright APIRequest ì‚¬ìš©
                            api_request = context.request
                            
                            # 1. í—¤ë” ì—†ì´ ìš”ì²­
                            resp = await api_request.get(test_url)
                            ct = resp.headers.get('content-type', '')
                            print(f"   [1ì°¨ ì‹œë„] Status: {resp.status} | Type: {ct}")
                            
                            if resp.status != 200 or 'pdf' not in ct.lower():
                                # 2. Referer í—¤ë” ì¶”ê°€ ìš”ì²­
                                print("   âš ï¸ 1ì°¨ ì‹¤íŒ¨/ì˜ì‹¬ -> Referer í—¤ë” ì¶”ê°€í•˜ì—¬ 2ì°¨ ì‹œë„...")
                                resp = await api_request.get(test_url, headers={
                                    "Referer": referer_url,
                                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                                })
                                ct = resp.headers.get('content-type', '')
                                print(f"   [2ì°¨ ì‹œë„] Status: {resp.status} | Type: {ct}")
                                
                                if resp.status == 200 and ('pdf' in ct.lower() or 'octet-stream' in ct.lower()):
                                    print("   âœ… Referer í—¤ë”ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤! (ë‹¤ìš´ë¡œë“œ ì‹œ Refererë¥¼ í¬í•¨í•´ì•¼ í•¨)")
                                else:
                                    print("   âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (URLì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ì„¸ì…˜/ê¶Œí•œ ë¬¸ì œ)")
                            else:
                                print("   âœ… URL ì •ìƒ (ë³„ë„ ì¸ì¦ ì—†ì´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥)")
                                
                        except Exception as e:
                            print(f"   âŒ ê²€ì¦ ì¤‘ ì—ëŸ¬: {e}")
                # -----------------------------
                
                # CSV ì €ì¥
                output_dir = r"D:\Antigravity\coding\output"
                os.makedirs(output_dir, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filepath = os.path.join(output_dir, f"ricon_{timestamp}.csv")
                
                with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=['source', 'title', 'date', 'page_url', 'pdf_url', 'collected_at'])
                    writer.writeheader()
                    writer.writerows(scraper.results)
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
            else:
                print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await context.close()
            await browser.close()

if __name__ == "__main__":
    asyncio.run(test_ricon_scraper())
