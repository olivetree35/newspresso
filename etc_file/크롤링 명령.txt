🧠 AI에게 사이트 데이터 수집을 지시하기 위한
📌 크롤링 방법 · 필수 용어 · 명령어 치트시트 (복붙용)

────────────────────────────────

[1] 데이터 수집 방법 전체 분류

1. RSS / 피드 수집
2. 정적 HTML 크롤링 (requests + BeautifulSoup)
3. 동적 페이지 크롤링 (Playwright / Selenium)
4. 백엔드 API 직접 호출
5. 사이트맵(sitemap.xml) 수집
6. 사이트 내 검색 결과 수집
7. 파일(PDF / 엑셀) 수집

────────────────────────────────

[2] 방법별 AI 명령어 템플릿

▶ A. RSS / 피드 수집
- 이 URL이 RSS인지 확인해줘.
- RSS 구조를 분석해서 title, link, published_at을 추출해줘.
- RSS item이 0개면 XML 전체 구조를 출력해서 원인을 분석해줘.

▶ B. 정적 HTML 크롤링
- View Source 기준으로 기사 목록이 있는 CSS 선택자를 찾아줘.
- requests + BeautifulSoup로 title, url, date를 추출하는 코드를 만들어줘.
- 상대경로 링크를 절대경로로 변환하는 로직(urljoin)을 포함해줘.
- 중복 제거 기준은 url 우선, 없으면 title+date로 해줘.

▶ C. 동적 페이지 크롤링 (JS 렌더링)
- 이 사이트가 정적인지 동적인지 View Source와 Elements 기준으로 판단해줘.
- Playwright로 로딩 완료 후 기사 목록을 수집하는 코드로 만들어줘.
- 무한스크롤 구조면 스크롤 반복해서 N개까지 수집해줘.
- 쿠키/팝업 배너가 있으면 자동으로 닫는 처리도 추가해줘.

▶ D. 백엔드 API 직접 호출
- Network 탭에서 JSON으로 내려오는 API 엔드포인트를 찾아줘.
- 해당 API를 직접 호출하는 방식으로 크롤링 구조를 바꿔줘.
- 페이지네이션 방식이 page/size인지 cursor 방식인지 분석해서 구현해줘.

▶ E. 사이트맵(sitemap.xml)
- 이 도메인에 sitemap.xml이 있는지 확인해줘.
- sitemap index가 있으면 하위 sitemap까지 모두 따라가서 URL을 수집해줘.
- 수집한 URL을 방문해 본문과 날짜를 추출하는 2단계 구조로 만들어줘.

▶ F. 사이트 내 검색 기반 수집
- 사이트 검색 결과 페이지의 URL 규칙을 분석해줘.
- 검색 결과 페이지네이션 규칙을 찾아 1~N페이지까지 수집해줘.

▶ G. 파일(PDF/엑셀) 수집
- 리스트 페이지에서 PDF 링크만 추출해서 다운로드해줘.
- PDF가 텍스트 기반인지 OCR이 필요한지 먼저 판단해줘.

────────────────────────────────

[3] 크롤링 필수 용어 (AI 지시용 최소 세트)

- 정적 페이지: View Source에 데이터 있음
- 동적 페이지: JS로 나중에 로딩
- CSS 선택자: 원하는 요소를 집는 규칙
- 페이지네이션: 다음 페이지 이동 규칙
- 무한 스크롤: 스크롤해야 더 로딩
- 헤더(Header): User-Agent, Referer
- 차단: 403, 429, 캡차
- 중복 제거: url 또는 title+date 기준

────────────────────────────────

[4] 에러 발생 시 점검 명령어

▶ 수집 결과 0개
- View Source에 기사 링크가 있는지 확인해줘. 없으면 동적 로딩으로 보고 Playwright로 전환해줘.
- CSS 선택자가 변경됐을 가능성이 있으니 후보 선택자 3개를 제안해줘.

▶ 403 / 401 / 차단
- User-Agent, Referer 헤더를 브라우저처럼 맞춰서 재시도해줘.
- 요청 간격, 랜덤 딜레이, 세션 유지(cookie) 로직을 추가해줘.

▶ 429 (요청 과다)
- 요청 속도를 줄이고 지수 백오프 방식 재시도를 추가해줘.

▶ 날짜 / 본문 파싱 오류
- 날짜 포맷이 여러 개인지 확인하고 dateutil로 파싱해줘.
- 본문 선택자 후보를 여러 개 두고 가장 긴 텍스트를 본문으로 선택해줘.

▶ 한글 깨짐
- 인코딩을 자동 감지해서 정상적인 한글로 디코딩해줘.

────────────────────────────────

[5] ⭐ 만능 AI 지시문 (URL만 바꿔서 사용)

아래 URL에서 데이터 수집을 하려고 해.

1) 정적 페이지인지 동적 페이지인지 먼저 판단하고
2) 정적이면 requests + BeautifulSoup,
   동적이면 Playwright 방식으로 진행하고
3) title, url, published_at, press, category를 추출하고
4) 페이지네이션 또는 무한스크롤 처리 포함
5) 중복 제거는 url 우선, 없으면 title+published_at 기준
6) 에러 발생 시 원인을 로그로 남기도록 코드로 만들어줘

URL: 여기에 사이트 주소 붙여넣기

────────────────────────────────

[한 줄 핵심]
크롤링은 "방법을 먼저 분류하고, 그 방법의 용어로 AI에게 지시"하면 된다.